This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
01_mcp_server__fastmcp__.md
02_settings___configuration_.md
03_browser_use_agent__browser_automation_orchestrator__.md
04_deep_research_agent_.md
05_custom_browser__playwright_wrapper__.md
06_custom_controller__action_registry__.md
07_llm_provider__language_model_integration__.md
index.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="01_mcp_server__fastmcp__.md">
# Chapter 1: MCP Server (FastMCP)

Imagine you want to use a smart assistant, like an AI chatbot, to do things for you online. Maybe you want it to "browse a website" or "research a topic." How does your assistant, which is just a bunch of computer code, actually *talk* to a web browser or a research tool?

This is where the **MCP Server (FastMCP)** comes in! Think of FastMCP as the central "switchboard operator" or "coordinator" for our browser automation system. Its main job is to listen for requests from your AI assistant and then connect those requests to the right tools and services within our system.

## The Core Idea: Bridging AI and Browser Actions

Without FastMCP, your AI assistant would have no simple way to tell our browser automation system what to do. It would be like trying to call a friend without a phone. FastMCP provides that phone system, allowing different AI assistants (that understand the "language" of MCP) to easily send commands to our browser tools.

The "MCP" in MCP Server stands for **Model Context Protocol**. This is a special "language" or set of rules that AI agents use to communicate with each other and with services like ours. FastMCP knows this language, making it easy for any AI agent that also speaks MCP to use our browser automation.

## How it Works: A Simple Scenario

Let's walk through a little story to see FastMCP in action.

**Use Case:** Your AI assistant wants to "Go to example.com and tell me the title of the page."

Here's what happens behind the scenes with FastMCP:

1.  **AI Assistant has a Request:** Your AI assistant decides it needs to use the web browser. It knows about a special "tool" or `run_browser_agent` capability that our system provides.
2.  **Assistant Talks to FastMCP:** Your AI assistant sends a message (following the MCP rules) to the FastMCP server, essentially saying: "Hey FastMCP, please run the `run_browser_agent` tool and ask it to get the title of `example.com`."
3.  **FastMCP Routes the Request:** FastMCP receives this message. It's like the switchboard operator saying, "Ah, `run_browser_agent`! I know who handles that!"
4.  **FastMCP Connects to the Right Tool:** FastMCP then passes this request to the actual `run_browser_agent` (which we'll learn more about in [Browser Use Agent (Browser Automation Orchestrator)](03_browser_use_agent__browser_automation_orchestrator__.md)).
5.  **Tool Does the Work:** The `run_browser_agent` then uses other parts of our system (like the browser itself) to visit `example.com` and find its title.
6.  **Tool Sends Result Back:** The `run_browser_agent` sends the title back to FastMCP.
7.  **FastMCP Sends Result to AI Assistant:** FastMCP relays the result (the page title) back to your original AI assistant.

This whole process allows your AI assistant to perform complex web tasks without needing to know *how* to control a browser directly. It just tells FastMCP *what* it wants done, and FastMCP handles the rest!

## Internal View: What's Under the Hood?

Let's visualize this with a simplified diagram:

```mermaid
sequenceDiagram
    participant AI as Your AI Assistant
    participant FMCP as FastMCP (Server)
    participant BUA as Browser Use Agent
    participant Browser as Custom Browser
    participant Web as Internet (example.com)

    AI->>FMCP: "Run browser agent: Go to example.com, get title."
    FMCP->>BUA: "Run task: Go to example.com, get title."
    BUA->>Browser: "Open tab, navigate to example.com."
    Browser->>Web: Request example.com
    Web-->>Browser: Page content (HTML/title)
    Browser-->>BUA: Page title received.
    BUA-->>FMCP: "Task complete. Title is 'Example Domain'."
    FMCP-->>AI: "Result: Title is 'Example Domain'."
```

As you can see, FastMCP is `FMCP` in this diagram. Itâ€™s the central hub for requests.

### Looking at the Code (Simplified)

In our project, the main setup for the MCP Server is handled in `src/mcp_server_browser_use/server.py`.

Here's a tiny glimpse of how `FastMCP` (the core component of our MCP Server) is used to set up the server:

```python
# src/mcp_server_browser_use/server.py (Simplified)

from mcp.server.fastmcp import FastMCP # This is the core FastMCP library

# ... other imports and setup ...

def serve() -> FastMCP:
    """Configures and returns the FastMCP server instance."""
    server = FastMCP(
        server_id="browser-use", # A unique name for our server
        # ... other configurations for our server ...
    )

    # Register the run_browser_agent tool
    @server.tool(
        name="run_browser_agent",
        description="Tool for general browser automation and interaction...",
    )
    async def run_browser_agent(
        ctx: Context,  # Information about the incoming request
        task: str,     # The instruction from the AI assistant (e.g., "Go to example.com")
        # ... other parameters ...
    ) -> str:
        # This is where we will call our Browser Use Agent later!
        # For now, imagine it does the work:
        logger.info(f"AI requested browser agent task: {task}")
        # result = await some_browser_agent_function(task, ctx, config)
        return f"Browser agent successfully processed task: {task}"

    # We also register the deep research tool here!
    @server.tool(
        name="run_deep_research",
        description="Tool for multi-step web research and report generation...",
    )
    async def run_deep_research(
        ctx: Context,
        research_task: str, # The research question from AI assistant
        # ... other parameters ...
    ) -> str:
        # This is where we will call our Deep Research Agent later!
        logger.info(f"AI requested deep research: {research_task}")
        return f"Deep research processed: {research_task}"

    return server

# This line actually creates and starts our server when the program runs
server_instance = serve()
```

This code snippet shows:
*   We use something called `FastMCP` from a library to create our server.
*   We give our server a unique `server_id` like `"browser-use"`.
*   We tell `FastMCP` about special "tools" it provides. In our case, `run_browser_agent` and `run_deep_research`. These are like buttons on our "switchboard" that AI assistants can "press."
*   When an AI assistant uses one of these tools, the code inside the `@server.tool` function (like `run_browser_agent` or `run_deep_research`) will run. This is where we connect to the actual browser automation and research parts of our system!

You can see these `run_browser_agent` and `run_deep_research` calls referenced in the `README.md` file's "Features" section. This is how the system makes its capabilities available to other AI agents.

## Why MCP?

The Model Context Protocol (MCP) ensures that different AI frontends (like Claude Desktop, mentioned in the README) can easily integrate with our `browser-use` server. It's a standardized way for them to understand what tools we offer and how to use them.

The `README.md` file shows how an MCP client (like Claude Desktop) would configure itself to use our `browser-use` server:

```json
// Example 1: One-Line Latest Version (Always Fresh)
"mcpServers": {
    "browser-use": { // This "browser-use" matches our server_id!
      "command": "uvx",
      "args": ["mcp-server-browser-use@latest"],
      "env": {
        // ... environment variables ...
      }
    }
}
```

This configuration tells the MCP client how to start our `browser-use` server and what environment variables it needs (like API keys for LLMs, which we'll discuss in [LLM Provider (Language Model Integration)](07_llm_provider__language_model_integration__.md)).

## Conclusion

You've just learned about the brain of our browser automation system: the **MCP Server (FastMCP)**. It acts as the crucial link, allowing AI assistants to request web browsing and research tasks and then routing those requests to the right internal tools. It does this by implementing the Model Context Protocol (MCP) for standardized communication.

In the next chapter, we'll dive into [Settings & Configuration](02_settings___configuration_.md), where we'll see all the different ways you can customize how your MCP Server and its tools behave!

[Next Chapter: Settings & Configuration](02_settings___configuration_.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="02_settings___configuration_.md">
# Chapter 2: Settings & Configuration

In the [previous chapter](01_mcp_server__fastmcp__.md), we learned that the FastMCP server is like the central switchboard for our browser automation system. It listens for requests from an AI assistant and routes them to the right tools. But how does this switchboard know *how* to operate its tools? For example, should the browser open visibly or secretly (headless)? Where should it save files?

This is where **Settings & Configuration** comes in! Think of it like the dashboard of a car. Just as you use knobs and buttons on a car's dashboard to control things like the lights, climate, or choose between drive and reverse, our settings allow you to control how the entire `mcp-browser-use` application behaves. Itâ€™s the control panel for everything from which AI model to use to how the browser operates.

## Why Do We Need Settings?

Imagine you're building a house. You could nail every piece of wood directly, or you could use screws. Screws allow you to adjust things later, move walls if needed, or even dismantle parts and rebuild. Settings are like the "screws" of our application. They make the system flexible and adaptable without having to change the core "structure" (the code).

**Central Use Case:** You want to run a web research task, but you want to see the browser opening on your screen (not hidden). Also, you want to store all downloaded files in a specific folder. How do you tell the system that? By changing its settings!

## Key Concepts of Settings

Our settings are organized into different groups, just like a car's dashboard has climate controls, media controls, and driving controls. Each group handles a specific part of the system.

In `mcp-browser-use`, our settings are defined in a file called `src/mcp_server_browser_use/config.py`. Let's look at the main groups:

1.  **LLM Settings (`LLMSettings`)**:
    *   **What it controls**: Which Large Language Model (LLM) the system uses (e.g., Google's Gemini, OpenAI's GPT). Also, things like its "temperature" (how creative it should be) and API keys.
    *   **Analogy**: This is like choosing your car's engine type (gas, electric, hybrid) and how aggressively it should accelerate.

2.  **Browser Settings (`BrowserSettings`)**:
    *   **What it controls**: How the web browser behaves. For example, if it should be `headless` (run in the background, invisible) or `false` (open a visible window). Where to save user data, or even connect to an already open browser.
    *   **Analogy**: This controls your car's driving mode (sport, economy, comfortable), whether its lights are on, or if you use cruise control.

3.  **Agent Tool Settings (`AgentToolSettings`)**:
    *   **What it controls**: Specific behaviors of the AI agents that use the browser. For example, how many steps an agent can take, or if it should use "vision" (looking at screenshots of web pages).
    *   **Analogy**: These are advanced features on your car's dashboard, like lane-keeping assist or adaptive cruise control, which guide how the car performs certain driving tasks.

4.  **Research Tool Settings (`ResearchToolSettings`)**:
    *   **What it controls**: How the deep research agent operates. This includes where it saves its research reports and how many browsers it can use at once.
    *   **Analogy**: This is like setting your car's navigation: where to save recent destinations or how many alternative routes to consider for speed.

5.  **Path Settings (`PathSettings`)**:
    *   **What it controls**: Where various temporary or output files are stored, such as downloaded files.
    *   **Analogy**: This tells the car's navigation where your "Home" or "Work" addresses are saved.

## How to Configure Settings

The `mcp-browser-use` application uses a popular Python library called `Pydantic-Settings` to manage its configuration. This library is very smart because it can read settings from multiple places, in a specific order:

1.  **Hardcoded Defaults**: These are the default values already written in `src/mcp_server_browser_use/config.py`.
2.  **`.env` files**: You can create a file named `.env` in your project's root directory and put settings there. This is very common for local development.
3.  **Environment Variables**: You can set these directly in your operating system or command line before running the application.

**Let's solve our use case:** You want a visible browser and specific download folder.

We will use an `.env` file because it's easy to manage and good for keeping settings separate from your code.

1.  **Create a `.env` file**: In the root of your `mcp-browser-use` project folder (the same folder where `src` is), create a new file named `.env`.

2.  **Add your settings to `.env`**:
    ```dotenv
    # .env file
    MCP_BROWSER_HEADLESS=False
    MCP_PATHS_DOWNLOADS=/path/to/your/custom/downloads # On Windows, might be C:\Users\YourUser\Downloads
    ```
    *   `MCP_BROWSER_HEADLESS=False`: This tells the `BrowserSettings` group that the `headless` option should be `False`. This means the browser will open visibly.
        *   `MCP_BROWSER_` is the prefix for `BrowserSettings`.
        *   `HEADLESS` is the name of the setting within `BrowserSettings`.
    *   Change `/path/to/your/custom/downloads` to an actual folder on your computer where you want files to be saved. For example, on Linux/macOS, it could be `/tmp/my_browser_downloads`, and on Windows, `C:\my_browser_downloads`.

    **Explanation**: The `Pydantic-Settings` library automatically looks for environment variables that start with `MCP_BROWSER_` for `BrowserSettings`, `MCP_PATHS_` for `PathSettings`, and so on. The `_` separates the prefix from the actual setting name.

Now, when you run the `mcp-browser-use` server or CLI (which we'll cover later), it will automatically load these settings from your `.env` file!

## How Settings Work Under the Hood

Let's peek at the file `src/mcp_server_browser_use/config.py` to understand how these settings are defined and loaded.

### Defining Setting Groups

Each group of settings is defined as a Python class that inherits from `BaseSettings`.

```python
# src/mcp_server_browser_use/config.py (simplified)

from typing import Optional
from pydantic import Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

class LLMSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="MCP_LLM_") # Important: This defines the prefix!

    provider: str = Field(default="google", env="PROVIDER") # MCP_LLM_PROVIDER
    model_name: str = Field(default="gemini-2.5-flash-preview-04-17", env="MODEL_NAME") # MCP_LLM_MODEL_NAME
    temperature: float = Field(default=0.0, env="TEMPERATURE") # MCP_LLM_TEMPERATURE
    api_key: Optional[SecretStr] = Field(default=None, env="API_KEY") # MCP_LLM_API_KEY
    # ... more LLM settings ...

class BrowserSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="MCP_BROWSER_") # Its prefix is MCP_BROWSER_

    headless: bool = Field(default=False, env="HEADLESS") # MCP_BROWSER_HEADLESS
    disable_security: bool = Field(default=False, env="DISABLE_SECURITY") # MCP_BROWSER_DISABLE_SECURITY
    binary_path: Optional[str] = Field(default=None, env="BINARY_PATH") # MCP_BROWSER_BINARY_PATH
    # ... more browser settings ...

class PathSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="MCP_PATHS_") # Its prefix is MCP_PATHS_

    downloads: str = Field(default="./tmp/downloads", env="DOWNLOADS") # MCP_PATHS_DOWNLOADS
    recordings: str = Field(default="./tmp/recordings", env="RECORDINGS") # MCP_PATHS_RECORDINGS
    # ... more path settings ...

# And so on for AgentToolSettings, ResearchToolSettings, ServerSettings...
```
**Explanation**:
*   `class LLMSettings(BaseSettings):`: This creates a new group for LLM-related settings.
*   `model_config = SettingsConfigDict(env_prefix="MCP_LLM_")`: This is the magic line! It tells `Pydantic-Settings` to look for environment variables that start with `MCP_LLM_` (e.g., `MCP_LLM_PROVIDER`, `MCP_LLM_MODEL_NAME`) when filling in the values for this group.
*   `provider: str = Field(default="google", env="PROVIDER")`: This defines a setting named `provider`. Its default value is `"google"`. It will look for an environment variable named `MCP_LLM_PROVIDER`.

### Global Application Settings

All these individual setting groups are then combined into one large `AppSettings` class, which holds all the configuration for the entire application.

```python
# src/mcp_server_browser_use/config.py (simplified)

class AppSettings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",            # Look for a .env file
        env_file_encoding="utf-8",
        extra="ignore",             # Ignore extra env vars not defined
    )

    llm: LLMSettings = Field(default_factory=LLMSettings)
    browser: BrowserSettings = Field(default_factory=BrowserSettings)
    agent_tool: AgentToolSettings = Field(default_factory=AgentToolSettings)
    research_tool: ResearchToolSettings = Field(default_factory=ResearchToolSettings)
    paths: PathSettings = Field(default_factory=PathSettings)
    server: ServerSettings = Field(default_factory=ServerSettings)

# Create a single global instance of our settings
settings = AppSettings()
```
**Explanation**:
*   `class AppSettings(BaseSettings):`: This is the main class that combines all the smaller setting groups.
*   `env_file=".env"`: This tells `Pydantic-Settings` to automatically load variables from a file named `.env` if it exists.
*   `llm: LLMSettings = Field(default_factory=LLMSettings)`: This line means that `AppSettings` has a property called `llm`, which is an instance of `LLMSettings`. This is how all the specific settings are nested.

### Loading Settings in the Server

When the FastMCP server starts (as seen in `src/mcp_server_browser_use/server.py`), it simply imports the `settings` object that was created in `config.py`.

```python
# src/mcp_server_browser_use/server.py (simplified)

from .config import settings # Import global AppSettings instance

# ... later in the code ...

def serve() -> FastMCP:
    # ...
    # When creating a custom browser, it uses settings.browser.headless
    agent_headless_override = settings.agent_tool.headless
    browser_headless = agent_headless_override if agent_headless_override is not None else settings.browser.headless

    # When configuring deep research, it uses settings.research_tool.save_dir
    if settings.research_tool.save_dir:
        # If save_dir is provided, construct the full save directory path for this specific task
        save_dir_for_this_task = str(Path(settings.research_tool.save_dir) / task_id)

    # When configuring LLMs, it uses settings.llm.* values
    research_llm = internal_llm_provider.get_llm_model(settings.get_llm_config(is_planner=True)) # Example

    # ... and so on ...
```
**Explanation**:
This code shows how different parts of the application (like setting up the browser or configuring the LLM) access the `settings` object to get the values they need. Since the `settings` object has already loaded all the configurations from the `.env` file (or environment variables), the changes you made are now applied!

## Recap & Next Steps

You've learned that **Settings & Configuration** is the control panel for `mcp-browser-use`. It uses a clever system to let you adjust how the server, browser, and AI agents behave without touching the core code. By creating a simple `.env` file, you can easily customize vital behaviors, like making the browser visible or changing download locations. This makes the application incredibly flexible for different uses.

In the next chapter, we'll dive into the [Browser Use Agent (Browser Automation Orchestrator)](03_browser_use_agent__browser_automation_orchestrator__.md), the component that actually orchestrates browser actions based on the tasks given by the FastMCP server and guided by these very settings!

[Next Chapter: Browser Use Agent (Browser Automation Orchestrator)](03_browser_use_agent__browser_automation_orchestrator__.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="03_browser_use_agent__browser_automation_orchestrator__.md">
# Chapter 3: Browser Use Agent (Browser Automation Orchestrator)

In the [previous chapter](02_settings___configuration_.md), we configured the general behavior of our `mcp-browser-use` system. We learned how to set up things like whether the browser runs visibly (`headless=False`) or where files should be downloaded. But these settings are just *how* things are done. Now, let's talk about the "who" and the "what" â€“ the actual "brain" that tells the browser *what* to do based on a given task.

This chapter introduces the **Browser Use Agent (Browser Automation Orchestrator)**. Think of this agent as a very skilled project manager for browser tasks. You give it a high-level goal, like "find the price of 'widget X' on 'shopping-site.com'," and it figures out all the detailed steps: go to the site, search for "widget X," click on the product page, find the price, and report it back. It's the orchestrator that turns your general requests into specific actions in a web browser.

Let's imagine a central use case:

**Central Use Case:** You want the system to "Go to Wikipedia, search for 'Artificial Intelligence', and tell me the definition from the first paragraph."

The Browser Use Agent is the component that will make this happen!

## What is the Browser Use Agent?

The Browser Use Agent is the "brain" that drives the AI's interaction with the web browser. Picture a skilled driver: you tell them the destination, and they figure out the exact steps (turn here, press pedal, etc.). Similarly, the Browser Use Agent takes high-level tasks like "find information on a website" and breaks them down into specific browser actions (click buttons, type text, extract data) using its predefined tools. It manages the flow, retries, and overall strategy for web navigation.

It doesn't directly control the mouse and keyboard (that's handled by the [Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md)). Instead, it decides *what* actions should be taken and *when*.

## How the Browser Use Agent Works: A Simple Scenario

Let's revisit our use case and see how the Browser Use Agent (we'll call it BUA for short) handles it.

**Use Case:** "Go to Wikipedia, search for 'Artificial Intelligence', and tell me the definition from the first paragraph."

Here's how the BUA might break this down:

1.  **Understand the Goal:** The BUA receives the instruction: "Go to Wikipedia, search for 'Artificial Intelligence', and tell me the definition from the first paragraph."
2.  **Initial Action:** It decides the first step is to navigate to Wikipedia.
3.  **Observe:** After loading Wikipedia, it "looks" at the page (gets a snapshot of the page's structure and visible text).
4.  **Plan:** It sees a search bar. Its plan becomes: type "Artificial Intelligence" into the search bar.
5.  **Act:** It performs the "type" action.
6.  **Observe:** It "looks" at the page again after searching (now on the AI page).
7.  **Plan:** It sees the first paragraph and knows it needs to extract its text.
8.  **Act:** It performs the "extract text" action.
9.  **Report:** It sends the extracted definition back.

This "Observe -> Plan -> Act" loop is at the heart of the Browser Use Agent. It constantly evaluates the current state of the browser and decides the next best action to achieve the overall goal.

## How to use the Browser Use Agent (via CLI)

You can interact with the Browser Use Agent directly through the command line interface (CLI) of `mcp-browser-use`. This is a great way to test its capabilities.

First, make sure you have your `.env` file set up from the [previous chapter](02_settings___configuration_.md). For this example, it's useful to have a visible browser, so ensure `MCP_BROWSER_HEADLESS=False` is in your `.env` file. You'll also need to set up your LLM API key, like `MCP_LLM_API_KEY=your_google_api_key` if you are using Google's Gemini.

Let's try our use case with the CLI:

```bash
python -m src.mcp_server_browser_use.cli run-browser-agent "Go to Wikipedia, search for 'Artificial Intelligence', and tell me the definition from the first paragraph."
```

**What happens:**

1.  The `mcp-browser-use` CLI starts.
2.  It loads the settings, including your LLM API key and browser settings.
3.  It initialises the `BrowserUseAgent` behind the scenes.
4.  A web browser window will open (if `headless=False`).
5.  You'll see the browser navigating to Wikipedia.
6.  The agent will type "Artificial Intelligence" into the search bar.
7.  It will navigate to the AI page.
8.  It will extract the first paragraph.
9.  The extracted definition will be printed in your terminal. You might also see intermediate steps logged in the terminal as the agent thinks and acts.

This simple command tells the *entire* system to: start a browser, navigate, interact, and extract information â€“ all orchestrated by the Browser Use Agent.

## Inside the Browser Use Agent: Under the Hood

The Browser Use Agent is quite a sophisticated piece of software, but its core loop is simple: **Observe -> Plan -> Act**.

### The Flow: A High-Level Diagram

```mermaid
sequenceDiagram
    participant AI as Your AI Request (e.g., CLI/MCP Server)
    participant BUA as Browser Use Agent
    participant LLM as Language Model (e.g., Gemini/GPT)
    participant Browser as Custom Browser
    participant Web as Internet

    AI->>BUA: "Task: Find AI definition on Wikipedia"
    BUA->>Browser: "Navigate to wikipedia.org"
    Browser->>Web: Request page
    Web-->>Browser: Wikipedia page content
    Browser-->>BUA: Snapshot of Wikipedia page (DOM/screenshot)
    BUA->>LLM: "Given this page, and my goal, what's next? (e.g., find search bar)"
    LLM-->>BUA: "Thought: Find search bar. Action: Type 'Artificial Intelligence' into it."
    BUA->>Browser: "Type 'Artificial Intelligence' into element #search-input"
    Browser->>Web: Submit search
    Web-->>Browser: AI Wikipedia page content
    Browser-->>BUA: Snapshot of AI page
    BUA->>LLM: "Given this page, and my goal, what's next? (e.g., extract first paragraph)"
    LLM-->>BUA: "Thought: Extract first paragraph. Action: Get text from first paragraph."
    BUA->>Browser: "Extract text from CSS selector 'p:first-of-type'"
    Browser-->>BUA: Extracted definition text
    BUA-->>AI: "Result: [The definition you asked for]"
```
As you can see, the Browser Use Agent (BUA) is constantly talking to the [Language Model Integration (LLM Provider)](07_llm_provider__language_model_integration__.md) to decide its next step and then telling the [Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md) what to do.

### Core Components in Code

The main logic for the `BrowserUseAgent` lives in `src/mcp_server_browser_use/_internal/agent/browser_use/browser_use_agent.py`.

Let's look at a simplified version of its `run` method, which orchestrates the "Observe -> Plan -> Act" loop.

```python
# src/mcp_server_browser_use/_internal/agent/browser_use/browser_use_agent.py (Simplified)

class BrowserUseAgent(Agent): # Agent is a base class from 'browser_use' library
    async def run(self, max_steps: int = 100, **kwargs) -> AgentHistoryList:
        """Execute the task with maximum number of steps"""
        # ... setup and logging ...

        for step in range(max_steps):
            # 1. Observe: Get the current state of the browser
            # self.state holds information like current browser state, history etc.
            # This involves getting screenshots and DOM info from the Custom Browser
            # (Details handled by self._get_context_for_llm())

            # 2. Plan (LLM plays a big part here):
            # This is where the LLM is asked: "Given the current browser state,
            # what's the next best action to achieve the task?"
            # The agent decides which tools to use.
            # For example, it might decide to use a 'type' tool, a 'click' tool, or 'extract_text' tool.
            await self.step(AgentStepInfo(step_number=step, max_steps=max_steps))

            # 3. Act: Execute the planned action
            # The agent performs the action suggested by the LLM by calling
            # methods in the Custom Controller (which talks to the browser).
            # (Details handled inside self.step() and related methods)

            if self.state.history.is_done(): # Agent decided task is complete
                break
        # ... cleanup and result return ...
        return self.state.history
```
**Explanation:**

*   `BrowserUseAgent(Agent)`: Our `BrowserUseAgent` builds upon a general `Agent` class from the `browser-use` library, which provides the basic structure for agents.
*   `run()`: This is the main function you call (e.g., from the CLI or the [MCP Server (FastMCP)](01_mcp_server__fastmcp__.md)) to start the agent. It contains the core loop.
*   `for step in range(max_steps)`: The agent attempts to complete the task within a certain number of steps to prevent infinite loops.
*   `await self.step(...)`: This is the crucial part. Inside this method, the agent:
    *   Gets a snapshot of the current browser page (visuals and structure) from the [Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md).
    *   Sends this information, along with the task an its history, to the [LLM Provider (Language Model Integration)](07_llm_provider__language_model_integration__.md).
    *   The LLM "thinks" and recommends the next action (e.g., "click button X," "type into field Y").
    *   The agent then uses the [Custom Controller (Action Registry)](06_custom_controller__action_registry__.md) to actually perform that action in the browser.
*   `self.state.history.is_done()`: The agent will eventually decide if it has successfully completed the task or if it cannot proceed.

### Prompting the LLM (Simplified)

The crucial part of the "Plan" stage is how the Browser Use Agent talks to the LLM. It gives the LLM context about the current task, the browser's state, and available tools.

```python
# src/mcp_server_browser_use/_internal/agent/browser_use/browser_use_agent.py (Simplified)

class BrowserUseAgent(Agent):
    async def _get_action_from_llm(self, message_manager: MessageManager, system_prompt: SystemPrompt) -> ActionModel:
        """
        Asks the LLM to decide the next action based on the current state.
        """
        # 1. Get current browser snapshot
        current_browser_state: BrowserState = await self.browser_context.get_current_state()

        # 2. Prepare the messages for the LLM
        # This includes the system prompt (rules, tools), the task, and the current browser state.
        messages: List[BaseMessage] = message_manager.build_messages(
            system_prompt=system_prompt,
            current_browser_state=current_browser_state
        )

        # 3. Call the LLM (e.g., Gemini, GPT)
        # The LLM is expected to return a JSON object that matches our ActionModel
        llm_output_str: str = await self.llm.invoke(messages)

        # 4. Parse the LLM's output into a structured action
        action_model = extract_json_from_model_output(llm_output_str, ActionModel)
        return action_model
```
**Explanation:**

*   `_get_action_from_llm()`: This internal helper function shows how the agent interacts with the LLM.
*   `self.browser_context.get_current_state()`: This gets the detailed information about what the browser currently "sees" (e.g., the URL, visible text, links, buttons, and potentially a screenshot). This comes from the [Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md).
*   `message_manager.build_messages()`: This prepares the conversation history and the detailed "system prompt" for the LLM. The system prompt tells the LLM its role, the available tools (like `click`, `type`, `extract_text`), and how to format its response.
*   `self.llm.invoke(messages)`: This is where the call to the actual LLM (e.g., Google's Gemini or OpenAI's GPT) happens. This uses our [LLM Provider (Language Model Integration)](07_llm_provider__language_model_integration__.md).
*   `extract_json_from_model_output(...)`: The LLM's raw text response is parsed to extract a structured action (e.g., `{"action": "click", "element_selector": "#search-button"}`), which then tells the agent what to do next.

## Conclusion

The **Browser Use Agent (Browser Automation Orchestrator)** is the core intelligence that translates your high-level web tasks into a series of detailed browser actions. It continuously observes the browser's state, plans the next optimal action using its understanding and the power of a Large Language Model, and then executes that action. It's the orchestrator that makes autonomous web interaction possible.

In the next chapter, we'll look at the [Deep Research Agent](04_deep_research_agent_.md), which builds upon the Browser Use Agent to perform even more complex, multi-step web research tasks.

[Next Chapter: Deep Research Agent](04_deep_research_agent_.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="04_deep_research_agent_.md">
# Chapter 4: Deep Research Agent

In the [previous chapter](03_browser_use_agent__browser_automation_orchestrator__.md), we learned about the `Browser Use Agent`, which is great for performing specific tasks on a single website, like finding a definition on Wikipedia. But what if your task is much bigger? What if you need to research a complex topic that requires visiting many different websites, gathering information, and then putting it all together into a report?

This is where the **Deep Research Agent** comes in! Imagine a professional research librarian: you give them a topic, and they systematically search multiple sources, analyze information, and compile a detailed report. This agent does exactly that for you on the web. It goes beyond simple browsing by performing multi-step web searches, often using many "Browser Use Agents" (our project managers from the last chapter!), to gather in-depth information and then synthesize it into a final report. It can even save this report for you.

**Central Use Case:** You want to research "The impact of AI on job markets in the next 5 years" and get a comprehensive report.

The Deep Research Agent is designed to tackle this kind of complex, multi-page, multi-source research challenge.

## What is the Deep Research Agent?

The Deep Research Agent is like having a mini-team of specialized researchers. Instead of just going to one website to find a specific piece of data, it plans a full research strategy. It decides:

*   What questions need to be answered to cover the topic?
*   What keywords should be used for searching?
*   Which search results look most promising?
*   How to extract the relevant details from each promising website.
*   How to combine all the little pieces of information into one big, organized report.

It's "deep" because it digs into many sources, not just one, and then *synthesizes* (puts together) the information into a meaningful report, not just a collection of facts.

## How the Deep Research Agent Works: A Simple Scenario

Let's break down our central use case: "Research 'The impact of AI on job markets in the next 5 years' and get a comprehensive report."

Here's how the Deep Research Agent might approach this:

1.  **Understand the Topic:** It first analyzes your broad research topic to identify key sub-questions. For example: "What types of jobs will be affected?", "Will new jobs be created?", "What economic impacts are predicted?".
2.  **Plan Searches:** For each sub-question, it will create search queries (e.g., "AI job displacement", "AI job creation", "economic impact of AI").
3.  **Execute Searches (using Browser Use Agents):**
    *   It will perform a search on a search engine (like Google).
    *   For each promising search result, it might launch a *separate* [Browser Use Agent](03_browser_use_agent__browser_automation_orchestrator__.md) to visit that website, gather information, and summarize it.
    *   It can do this for many search results at the same time (in parallel) to speed up research.
4.  **Gather & Combine Information:** As information comes back from each individual browsing task, the Deep Research Agent stores it.
5.  **Synthesize Report:** Once enough information is gathered, it uses an LLM to analyze all the collected data, identify patterns, resolve conflicts (if any), and write a coherent, detailed report on the original topic.
6.  **Save Report (Optional):** It can then save this final report to a specific folder on your computer.

This multi-step, multi-agent approach is what makes the Deep Research Agent so powerful for complex research.

## How to use the Deep Research Agent (via CLI)

Just like the `Browser Use Agent`, you can run the `Deep Research Agent` directly from the command line.

You still need your `.env` file from [Chapter 2](02_settings___configuration_.md) with your LLM API key. For deep research, it's highly recommended to set a `save_dir`. If you don't, the report will only be printed to the console (and not saved as a file).

Add this to your `.env` file (if you haven't already):
```dotenv
# .env file
MCP_LLM_API_KEY=your_google_api_key_here
MCP_RESEARCH_TOOL_SAVE_DIR="./tmp/deep_research_reports"
```
Remember to change `your_google_api_key_here` to your actual API key. The `MCP_RESEARCH_TOOL_SAVE_DIR` specifies where the agent should save its generated reports. It's a good idea to put this inside your project's root for organization, like `tmp/deep_research_reports`.

Now, let's run our central use case:

```bash
python -m src.mcp_server_browser_use.cli run-deep-research "The impact of AI on job markets in the next 5 years"
```

**What happens:**

1.  The `mcp-browser-use` CLI starts.
2.  It uses the settings, including your LLM API key and the save directory.
3.  The `DeepResearchAgent` is initialized.
4.  You'll see messages in your terminal as the agent plans its research.
5.  Multiple browser windows might open (if `headless=False` in your settings) as individual [Browser Use Agents](03_browser_use_agent__browser_automation_orchestrator__.md) perform parallel searches and data extraction.
6.  The agent gathers all the information.
7.  Finally, it synthesizes a report.
8.  The full report will be printed to your terminal, and a Markdown file (`report.md`) will be saved in a new subfolder within `tmp/deep_research_reports` (e.g., `tmp/deep_research_reports/your_task_id/report.md`).

This single command kicks off a powerful, multi-stage research process!

## Inside the Deep Research Agent: Under the Hood

The Deep Research Agent is much more complex than the `Browser Use Agent` because it coordinates multiple sub-agents and manages a multi-step research workflow. It uses a concept called **Langgraph** to manage this complex flow. Think of Langgraph as a flowchart designer for AI agents.

### The Flow: A High-Level Diagram

```mermaid
sequenceDiagram
    participant AI as Your AI Request (e.g., CLI/MCP Server)
    participant DRA as Deep Research Agent
    participant LLM as Language Model (e.g., Gemini/GPT)
    participant BUA as Browser Use Agent (many)
    participant FS as File System

    AI->>DRA: "Research 'AI Job Market Impact'"
    DRA->>LLM: "Help me create research questions & search terms."
    LLM-->>DRA: Research Plan (questions, keywords)
    loop For each research question
        DRA->>LLM: "Generate search queries for this question."
        LLM-->>DRA: Search Queries
        loop For each search query
            DRA->>BUA: "Search web, extract info for query."
            BUA->>LLM: (Observe-Plan-Act Loop from Ch 3)
            BUA->>Browser: (Web interactions)
            Browser-->>BUA: Page Info
            BUA-->>DRA: Extracted Search Result Info
        end
    end
    DRA->>LLM: "Synthesize all collected info into a report."
    LLM-->>DRA: Final Report Content
    DRA->>FS: "Save Report to file"
    FS-->>DRA: Report Saved
    DRA-->>AI: "Research complete. Report saved at /path/to/report.md"
```

As you can see, the Deep Research Agent (DRA) acts as the main conductor, orchestrating the LLM and spawning multiple [Browser Use Agents](03_browser_use_agent__browser_automation_orchestrator__.md) to gather information.

### Core Components in Code

The main logic for the `DeepResearchAgent` lives in `src/mcp_server_browser_use/_internal/agent/deep_research/deep_research_agent.py`.

It leverages a powerful library called `langgraph` to define its multi-stage process. Langgraph allows us to define "nodes" (steps) and "edges" (transitions between steps) in a graph, making complex workflows manageable.

Here's a very simplified look at how the `DeepResearchAgent` might set up its graph:

```python
# src/mcp_server_browser_use/_internal/agent/deep_research/deep_research_agent.py (Simplified)

class DeepResearchAgent:
    def __init__(self, **kwargs):
        # ... (setup LLM, browser config, etc.) ...
        self.graph = self._build_research_graph() # This builds the flowchart

    def _build_research_graph(self):
        workflow = StateGraph(ResearchState) # ResearchState holds current progress

        # Define nodes (steps in our research process)
        workflow.add_node("plan_research", self.plan_research_node)
        workflow.add_node("execute_search", self.execute_search_node)
        workflow.add_node("process_search_results", self.process_search_results_node)
        workflow.add_node("synthesize_report", self.synthesize_report_node)

        # Define edges (how steps connect)
        workflow.set_entry_point("plan_research") # Start here

        # After planning, go to execute search, then process results
        workflow.add_edge("plan_research", "execute_search")
        workflow.add_edge("execute_search", "process_search_results")

        # After processing, decide if more research is needed or if we can synthesize
        workflow.add_conditional_edges(
            "process_search_results",
            self.should_continue_research, # A function that decides where to go next
            {"continue": "execute_search", "finish": "synthesize_report"},
        )

        # After synthesizing, we are done
        workflow.add_edge("synthesize_report", END)

        return workflow.compile() # Turn the flowchart into runnable code

    async def run(self, topic: str, save_dir: str = None, **kwargs):
        initial_state = {"topic": topic, "save_dir": save_dir, "research_plan": [], "search_results": []}
        final_state = await self.graph.ainvoke(initial_state) # Run the flowchart
        # ... process final_state and save report ...
        return {"final_report": final_state.get("final_report", "No report generated.")}
```

**Explanation:**

*   `StateGraph(ResearchState)`: This tells Langgraph that our research process will keep track of its progress in a special `ResearchState` object (which stores the topic, plan, results, etc.).
*   `add_node(...)`: Each `add_node` line defines a distinct step in the research process. For instance:
    *   `plan_research_node`: This step uses the LLM to break down the main topic.
    *   `execute_search_node`: This step generates search queries and dispatches them to individual [Browser Use Agents](03_browser_use_agent__browser_automation_orchestrator__.md) (which run in parallel).
    *   `process_search_results_node`: This step takes the raw results from the browser agents and refines them.
    *   `synthesize_report_node`: This step combines all the refined information into the final report.
*   `set_entry_point("plan_research")`: This tells the graph to start at the "plan research" step.
*   `add_edge("plan_research", "execute_search")`: This means after `plan_research` is done, the graph automatically moves to `execute_search`.
*   `add_conditional_edges(...)`: This is smart! After `process_search_results`, the `should_continue_research` function decides if we need to do more searches (`"continue"`) or if we have enough info to write the report (`"finish"`).
*   `self.graph.ainvoke(initial_state)`: This line actually starts the entire research process, running through all the nodes and edges defined in the graph.

### Running Parallel Browser Tasks

A key feature of the Deep Research Agent is its ability to run multiple browser tasks at once to speed up data gathering. When the `execute_search_node` is run, it doesn't just call one `BrowserUseAgent`; it can call many!

The actual implementation uses Python's `asyncio` and `concurrent.futures` to manage these parallel tasks. Here's a tiny snippet showing the concept:

```python
# src/mcp_server_browser_use/_internal/agent/deep_research/deep_research_agent.py (Simplified)

async def execute_search_node(self, state: ResearchState):
    # ... generate search queries based on the plan ...

    search_queries_for_this_round = state.get("next_queries", [])
    logger.info(f"Executing {len(search_queries_for_this_round)} search tasks in parallel.")

    tasks = []
    for query in search_queries_for_this_round:
        # For each query, create a task to run a single BrowserUseAgent
        task_id = f"{state['task_id']}_subtask_{uuid.uuid4().hex[:6]}" # Unique ID for subtask
        tasks.append(
            # This function runs a BrowserUseAgent inside its own context
            run_single_browser_task(
                query,
                task_id=task_id,
                llm=self.llm, # Pass the LLM
                browser_config=self.browser_config,
                stop_event=self.stop_event,
                use_vision=state.get("use_vision", False)
            )
        )
    # Run all these tasks concurrently and wait for them to finish
    results = await asyncio.gather(*tasks)

    # ... process results and update ResearchState ...
    return {"search_results": results} # Add results to the state
```

**Explanation:**

*   `execute_search_node(self, state: ResearchState)`: This is one of the nodes in our Langgraph flowchart.
*   `search_queries_for_this_round`: The agent has already used the LLM to figure out what searches to do.
*   `tasks.append(run_single_browser_task(...))`: For each search query, we create an `asyncio` task. The `run_single_browser_task` function is responsible for setting up and running one instance of the [Browser Use Agent](03_browser_use_agent__browser_automation_orchestrator__.md).
*   `await asyncio.gather(*tasks)`: This magic line tells Python to run all the tasks concurrently. It waits until *all* of them are finished before proceeding. This is how the Deep Research Agent can explore many websites at the same time!

## Conclusion

The **Deep Research Agent** is your go-to tool for comprehensive web research. It skillfully combines the power of Language Models for planning and synthesis with the web interaction capabilities of multiple [Browser Use Agents](03_browser_use_agent__browser_automation_orchestrator__.md), performing complex, multi-stage research tasks and delivering organized reports. You've seen how it breaks down a complex problem into manageable steps and executes them efficiently.

In the next chapter, we're going to zoom in on the browser itself and understand how it interacts with the web in a way that our agents can understand. We'll explore the [Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md).

[Next Chapter: Custom Browser (Playwright Wrapper)](05_custom_browser__playwright_wrapper__.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="05_custom_browser__playwright_wrapper__.md">
# Chapter 5: Custom Browser (Playwright Wrapper)

In the [previous chapter](04_deep_research_agent_.md), we saw how the `Deep Research Agent` orchestrates complex web research, even launching multiple `Browser Use Agents` to gather information. But have you ever wondered how these agents actually "see" and "touch" the web? How do they open a website, click buttons, or type text?

This is where the **Custom Browser (Playwright Wrapper)** comes in! Imagine the AI agents are like your brain, deciding what to do. The Custom Browser is like your hand, eye, and keyboard working together. It's the actual web browser that the AI agents control, allowing them to interact with websites as a human would. It uses a powerful tool called `Playwright` to do this, but with special custom additions.

**Central Use Case:** An AI agent needs to visit `example.com` and take a screenshot.

The Custom Browser is the core component that performs this action. It makes sure the browser is open, ready, and can follow instructions.

## What is the Custom Browser (Playwright Wrapper)?

Think of it as a remote-controlled robot hand that can interact with websites. When the AI agent (like the `Browser Use Agent`) says "go to this page," it's the Custom Browser that actually does it.

Here's why it's a "Custom Browser" and not just plain Playwright:

*   **Playwright:** This is the underlying tool that provides the basic "robot hand" capabilities (opening browsers, clicking, typing, etc.). It's like the raw motor and finger mechanisms.
*   **Custom Capabilities:** Our Custom Browser adds special features on top of Playwright, making it more flexible and useful for AI agents:
    *   **Connecting to an existing Chrome browser (CDP):** This means you can launch your own Chrome browser and our system can "plug into" it, controlling your active window. This is very cool if you want to see exactly what the AI is doing in *your* browser.
    *   **Handling specific configs:** It respects settings like `headless` (browser runs invisibly in the background) or `user data` (keeping your browser's history, cookies, and logins).
    *   **Managing browser contexts:** In Playwright, a "context" is like a fresh, isolated browser window or tab. Our Custom Browser can manage these for specialized tasks, ensuring one agent's actions don't interfere with another's.

## How to Use the Custom Browser (Conceptually)

You won't directly "use" the `Custom Browser` with a simple command like `run-browser-agent`. Instead, the `Browser Use Agent` (and by extension the `Deep Research Agent`) *uses* the Custom Browser behind the scenes.

Let's revisit our central use case: An AI agent needs to visit `example.com` and take a screenshot.

When you run a command like:
```bash
python -m src.mcp_server_browser_use.cli run-browser-agent "Go to example.com and take a screenshot."
```

Behind the scenes:

1.  The `MCP Server (FastMCP)` receives the request.
2.  It hands it off to the `Browser Use Agent`.
3.  The `Browser Use Agent` decides it needs a browser. It tells the system to get a `Custom Browser` instance.
4.  The `Custom Browser` launches a new browser (or connects to an existing one, based on your settings).
5.  The `Browser Use Agent` then instructs the `Custom Browser` instance to "go to `example.com`."
6.  Once the page loads, it instructs the `Custom Browser` to "take a screenshot."
7.  The `Custom Browser` performs these actions using Playwright and returns the screenshot data.

## Inside the Custom Browser: Under the Hood

The Custom Browser wraps Playwright's capabilities to provide a controlled environment for our AI agents.

### The Flow: A High-Level Diagram

```mermaid
sequenceDiagram
    participant BUA as Browser Use Agent
    participant CB as Custom Browser (Playwright Wrapper)
    participant PW as Playwright Library
    participant WWW as World Wide Web

    BUA->>CB: "Open browser."
    CB->>PW: Request new browser instance (e.g., Chromium)
    PW-->>CB: Browser created.
    BUA->>CB: "Go to example.com"
    CB->>PW: Navigate to URL
    PW->>WWW: Request example.com
    WWW-->>PW: Page content
    PW-->>CB: Page loaded.
    BUA->>CB: "Take screenshot."
    CB->>PW: Capture screenshot
    PW-->>CB: Screenshot data
    CB-->>BUA: Screenshot data forwarded.
```

### Core Components in Code

The main parts of the Custom Browser are defined in `src/mcp_server_browser_use/_internal/browser/custom_browser.py` and `src/mcp_server_browser_use/_internal/browser/custom_context.py`.

#### `CustomBrowser` Class (The Browser Manager)

This class is responsible for launching or connecting to the actual web browser.

```python
# src/mcp_server_browser_use/_internal/browser/custom_browser.py (Simplified)

from playwright.async_api import async_playwright, Playwright
from browser_use.browser.browser import Browser

class CustomBrowser(Browser):
    async def _setup_builtin_browser(self, playwright: Playwright):
        """
        Sets up and returns a Playwright Browser instance.
        Handles settings like headless mode, window size, etc.
        """
        # Decisions based on our Settings & Configuration (from Chapter 2)
        if self.config.headless:
            # Code to set up for invisible browsing
            pass
        else:
            # Code for visible browsing (adjust window size etc.)
            pass

        # Check if we should connect to an existing browser (CDP)
        if self.config.use_own_browser and self.config.cdp_url:
            # Connect to browser launched by user
            browser = await playwright.chromium.connect_over_cdp(self.config.cdp_url)
        elif self.config.user_data_dir:
            # Launch browser with specific user data (cookies, history)
            browser = await playwright.chromium.launch_persistent_context(
                self.config.user_data_dir, headless=self.config.headless
            )
        else:
            # Standard new browser launch
            browser = await playwright.chromium.launch(
                headless=self.config.headless,
                args=self.config.extra_browser_args
            )
        return browser

    async def new_context(self, config=None):
        """Creates a new browser context (like a new tab/window) within this browser."""
        # This will return an instance of CustomBrowserContext
        from .custom_context import CustomBrowserContext, CustomBrowserContextConfig
        merged_config = {**(self.config.model_dump() if self.config else {}), **(config.model_dump() if config else {})}
        return CustomBrowserContext(config=CustomBrowserContextConfig(**merged_config), browser=self)

```
**Explanation:**

*   `CustomBrowser(Browser)`: Our `CustomBrowser` inherits from a base `Browser` class provided by `browser-use` library.
*   `_setup_builtin_browser(self, playwright: Playwright)`: This is where the magic of starting or connecting to a browser happens.
    *   It checks `self.config.headless` (from [Settings & Configuration](02_settings___configuration_.md)) to decide if the browser should run visibly or invisibly.
    *   It checks `self.config.use_own_browser` and `self.config.cdp_url` to see if it should connect to a Chrome browser *you* launched. This is the `CDP Connection` feature from the `README.md`.
    *   It checks `self.config.user_data_dir` to launch a browser that keeps your user data (like a regular browser saving your info).
    *   Otherwise, it just launches a brand new, clean browser.
*   `new_context()`: This method is used to create a new "browser context."

#### `CustomBrowserContext` Class (The Tab/Window Manager)

Once a browser is launched, the `CustomBrowserContext` manages specific browser sessions, like individual tabs or isolated windows. This is where the "anti-detection" measures and cookie loading happen.

```python
# src/mcp_server_browser_use/_internal/browser/custom_context.py (Simplified)

import json
import os
from browser_use.browser.context import BrowserContext

class CustomBrowserContext(BrowserContext):
    async def _create_context(self, browser):
        """
        Creates a new Playwright context (like a private tab) and applies custom settings.
        """
        # If we connected to an existing CDP browser or are using a persistent context,
        # we might just use its default context instead of creating a new one.
        if self.browser.config.cdp_url or self.browser.config.browser_binary_path:
            context = browser.contexts[0]
        else:
            # Otherwise, create a fresh new context with specific settings
            context = await browser.new_context(
                no_viewport=True,
                user_agent=self.config.user_agent,
                record_video_dir=self.config.save_recording_path,
                # ... many other settings for the context ...
            )

        # Load cookies if specified in settings
        if self.config.cookies_file and os.path.exists(self.config.cookies_file):
            with open(self.config.cookies_file, 'r') as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                # ... handle invalid cookie values ...

        # Inject anti-detection scripts
        await context.add_init_script(
            """
            // Hide "webdriver" property to make it look less like an automated browser
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            // ... more anti-detection measures ...
            """
        )
        return context

```
**Explanation:**

*   `CustomBrowserContext(BrowserContext)`: This also inherits from a base class that handles common browser actions.
*   `_create_context(self, browser)`: This method is called to set up a new isolated browsing session.
    *   It checks if we're connected via `CDP` or using a `persistent context` (which would have its own default context already), in which case it reuses that. This means the AI agent can control your *currently open* browser session.
    *   Otherwise, it creates a brand new, isolated Playwright `context`. This is useful for `Deep Research Agent` tasks where you might want multiple isolated browsing sessions running in parallel.
    *   It loads cookies from a file if `self.config.cookies_file` is set, allowing the AI to browse with your logged-in sessions.
    *   It injects *anti-detection scripts*. Websites sometimes try to figure out if they are being browsed by a human or a robot. These scripts make our browser look more like a human-controlled one, helping to avoid being blocked.

Together, `CustomBrowser` and `CustomBrowserContext` provide the robust and flexible browser environment that our AI agents need to interact with the world wide web. When you see messages about "navigating to a URL" or "taking a screenshot," it's these components doing the heavy lifting using Playwright.

## Conclusion

You've just peeled back the curtain on the **Custom Browser (Playwright Wrapper)**, the "robot hand" of our AI agents! You learned that it uses `Playwright` to control web browsers, but adds powerful custom features like connecting to your existing Chrome browser, managing user data, and even making it harder for websites to detect that it's an automated browser. This foundational component is what allows the `Browser Use Agent` and `Deep Research Agent` to perform their tasks effectively.

In the next chapter, we'll explore the [Custom Controller (Action Registry)](06_custom_controller__action_registry__.md), which is the bridge that translates the AI's high-level plans into the low-level actions that this Custom Browser actually performs.

[Next Chapter: Custom Controller (Action Registry)](06_custom_controller__action_registry__.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="06_custom_controller__action_registry__.md">
# Chapter 6: Custom Controller (Action Registry)

In the [previous chapter](05_custom_browser__playwright_wrapper__.md), we explored the `Custom Browser (Playwright Wrapper)`, which is the actual web browser our AI agents use to perform actions. It's like the "hand" that can navigate to websites and take screenshots. But a hand needs instructions! Who tells the hand what to do next? Who makes sense of the AI's high-level thoughts and translates them into specific browser clicks and types?

This is where the **Custom Controller (Action Registry)** comes in! Imagine a maestro conducting an orchestra. The maestro doesn't play every instrument, but they know exactly which instrument needs to play, and when, to create the desired music. The Custom Controller is that maestro for our browser automation. It knows all the available browser actions and orchestrates them based on the AI's plan.

**Central Use Case:** The AI (specifically, the `Browser Use Agent` from [Chapter 3](03_browser_use_agent__browser_automation_orchestrator__.md)) decides it needs to "click on a button with index 10" on a web page.

The Custom Controller is the mastermind that will actually execute this "click" action in the browser.

## What is the Custom Controller (Action Registry)?

The Custom Controller is the central hub for all browser-related commands. It's like a library of actions that the AI agents can "borrow" from. When an AI agent decides it needs to perform an action (like clicking, typing, or extracting information), it passes that request to the Custom Controller.

Here's why it's a "Custom Controller" and an "Action Registry":

*   **Action Registry:** This is the "library" part. It keeps a list of all the specific browser actions it knows how to perform (e.g., `click`, `type`, `go_to_url`, `extract_text`). Each action has a name and instructions on how to use it.
*   **Custom:** We can add our own special actions to this registry beyond the standard ones. For example, we might add an action to upload a file or ask for human help. It also handles integrating with external MCP servers for even more actions!
*   **Controller:** This is the "maestro" part. When an AI agent requests an action, the Controller looks it up in its registry, gets the detailed instructions, and then tells the `Custom Browser` to perform that action.

## How the Custom Controller Works: A Simple Scenario

Let's revisit our central use case: The AI decides it needs to "click on a button with index 10."

Here's how the Custom Controller helps execute this:

1.  **AI Plans Action:** The [Browser Use Agent](03_browser_use_agent__browser_automation_orchestrator__.md) (after asking the LLM, as discussed in [Chapter 3](03_browser_use_agent__browser_automation_orchestrator__.md)) decides its next step is to "click on an element with index 10." It generates an `ActionModel` (a structured instruction) containing this.

2.  **Controller Receives Action:** The `Browser Use Agent` passes this `ActionModel` to the `Custom Controller`'s `act` method.

3.  **Controller Looks Up Action:** The `Custom Controller` receives the request to `"click"`. It looks into its "Action Registry" (its internal list of actions) and finds the specific function that handles "click" actions.

4.  **Controller Executes Action:** The `Custom Controller` then calls this "click" function, passing it the necessary details, like `index=10` and the current `browser_context` (which is managed by the `Custom Browser` from [Chapter 5](05_custom_browser__playwright_wrapper__.md)).

5.  **Browser Performs Action:** The "click" function, in turn, tells the `Custom Browser` to actually perform the click operation on the web page.

6.  **Result Returned:** The `Custom Browser` completes the click, and the result (e.g., "click successful") is passed back through the Controller to the `Browser Use Agent`.

## How to Use the Custom Controller (Conceptually)

You won't directly "use" the `Custom Controller` with a simple CLI command. Just like the `Custom Browser`, it's an internal component that the `Browser Use Agent` (and by extension the `Deep Research Agent`) *uses* behind the scenes as part of its "Act" phase.

When you run a command like:
```bash
python -m src.mcp_server_browser_use.cli run-browser-agent "Go to example.com, then click the 'Contact Us' button."
```

Behind the scenes:

1.  The CLI triggers the `MCP Server (FastMCP)`.
2.  The `MCP Server` sends the task to the `Browser Use Agent`.
3.  The `Browser Use Agent` starts its "Observe -> Plan -> Act" loop.
4.  In the "Act" phase, when it needs to "go to example.com" or "click the 'Contact Us' button," it uses the `Custom Controller` to call those specific actions. The `Custom Controller` then makes the `Custom Browser` perform them.

## Inside the Custom Controller: Under the Hood

The `Custom Controller` is defined in `src/mcp_server_browser_use/_internal/controller/custom_controller.py`. It inherits its core functionality from a library class called `Controller` and adds our specific customizations.

### The Flow: A High-Level Diagram

```mermaid
sequenceDiagram
    participant BUA as Browser Use Agent (AI)
    participant CC as Custom Controller (Maestro)
    participant CR as Action Registry (Library)
    participant CB as Custom Browser (Hand)
    participant WEB as Website

    BUA->>CC: "Execute action: click (index=10)"
    CC->>CR: "Lookup 'click' action"
    CR-->>CC: "Found 'click' function"
    CC->>CB: "Tell browser: click element at index 10"
    CB->>WEB: Perform click on element
    WEB-->>CB: Click successful / Page updated
    CB-->>CC: Action result
    CC-->>BUA: Action result
```

This diagram shows how the `Browser Use Agent` asks the `Custom Controller` to perform an action. The `Custom Controller` then uses its internal `Action Registry` to find the right function and tells the `Custom Browser` which steps to execute on the website.

### Core Components in Code

Let's look at the `CustomController` class and its key methods.

#### `CustomController` Initialization and Custom Actions

```python
# src/mcp_server_browser_use/_internal/controller/custom_controller.py (Simplified)

from browser_use.controller.service import Controller, ActionResult
from browser_use.controller.registry.service import RegisteredAction
import logging
import os

logger = logging.getLogger(__name__)

class CustomController(Controller):
    def __init__(self, exclude_actions: list[str] = [], output_model=None, ask_assistant_callback=None):
        super().__init__(exclude_actions=exclude_actions, output_model=output_model)
        self._register_custom_actions() # This is where we add our special actions!
        self.ask_assistant_callback = ask_assistant_callback # For human interaction
        self.mcp_client = None # For future MCP tool integration

    def _register_custom_actions(self):
        """Register all custom browser actions"""

        @self.registry.action(
            "When executing tasks, prioritize autonomous completion. However, if you encounter a definitive blocker..."
        )
        async def ask_for_assistant(query: str, browser: Any): # 'Any' for simplicity, it's BrowserContext
            """Allows the AI to ask a human for help when it's stuck."""
            if self.ask_assistant_callback:
                # This calls the provided callback, assuming it's an async function
                user_response = await self.ask_assistant_callback(query, browser)
                msg = f"AI ask: {query}. User response: {user_response['response']}"
                logger.info(msg)
                return ActionResult(extracted_content=msg, include_in_memory=True)
            else:
                return ActionResult(extracted_content="Human cannot help you. Try another way.",
                                    include_in_memory=True)

        @self.registry.action(
            'Upload file to interactive element with file path',
        )
        async def upload_file(index: int, path: str, browser: Any, available_file_paths: list[str]):
            """Handles uploading a file to an HTML input element."""
            if path not in available_file_paths:
                return ActionResult(error=f'File path {path} is not available')
            if not os.path.exists(path):
                return ActionResult(error=f'File {path} does not exist')

            # --- Simplified logic ---
            # In reality, this would locate the specific input field based on 'index'
            # and use browser.set_input_files(path) to upload the file.
            logger.info(f"Simulating file upload of {path} to element {index}")
            return ActionResult(extracted_content=f"Successfully uploaded {path} to element {index}", include_in_memory=True)

        # Other standard actions like click, type, go_to_url are registered by the base Controller class.
```
**Explanation:**

*   `class CustomController(Controller):`: Our custom controller inherits from a base `Controller` class (from the `browser-use` library). This base class already provides many standard actions like `click`, `type`, `go_to_url`, etc.
*   `self._register_custom_actions()`: This is our special method that adds *new* actions that are specific to `mcp-browser-use`.
*   `@self.registry.action(...)`: This is a "decorator" that tells the `Controller` to register the function below it as an available action. The string inside the decorator is the description of the action, which the LLM uses to understand what the tool does.
*   `ask_for_assistant`: This is a custom action that allows the AI to explicitly request human help if it encounters a blocker.
*   `upload_file`: This is another custom action that handles uploading files to a web page.
*   `browser: Any`: This parameter in the action functions represents the `BrowserContext` (from [Chapter 5](05_custom_browser__playwright_wrapper__.md)) that the action will operate on.

#### `act` Method: The Maestro's Command Center

This is the central method that the `Browser Use Agent` calls to execute an action.

```python
# src/mcp_server_browser_use/_internal/controller/custom_controller.py (Simplified)

from pydantic import BaseModel
from typing import Optional, Dict, Any

class ActionModel(BaseModel): # Simplified example of what an action model might look like
    click: Optional[Dict[str, Any]] = None # Example: {"index": 10}
    go_to_url: Optional[Dict[str, Any]] = None
    ask_for_assistant: Optional[Dict[str, Any]] = None
    # ... other potential actions

class CustomController(Controller):
    # ... (init and _register_custom_actions methods) ...

    async def act(
            self,
            action: ActionModel, # The instruction from the AI, e.g., {"click": {"index": 10}}
            browser_context: Any, # The current browser context from Custom Browser
            # ... other optional parameters ...
    ) -> ActionResult:
        """Execute an action requested by the AI."""
        try:
            # ActionModel can have multiple fields, but usually only one is set
            for action_name, params in action.model_dump(exclude_unset=True).items():
                if params is not None:
                    if action_name.startswith("mcp"):
                        # This would handle actions coming from other MCP servers
                        logger.debug(f"Invoking MCP tool: {action_name}")
                        # In reality, this would use a dynamically registered MCP client tool
                        return ActionResult(extracted_content=f"Simulating MCP tool '{action_name}' with {params}")
                    else:
                        # Find the action in our registry and execute it!
                        result = await self.registry.execute_action(
                            action_name, # e.g., "click"
                            params,      # e.g., {"index": 10}
                            browser=browser_context, # Pass the browser to the action
                            # ... other parameters ...
                        )
                        if isinstance(result, ActionResult):
                            return result
                        else: # Convert simple string results to ActionResult
                            return ActionResult(extracted_content=str(result))
            return ActionResult() # If no action specified, return empty
        except Exception as e:
            logger.error(f"Error executing action: {e}")
            raise e

```
**Explanation:**

*   `action: ActionModel`: This `ActionModel` is typically a `Pydantic` model generated by the LLM (based on the `SystemPrompt` giving it the available tools). It will have one field set, corresponding to the action the LLM wants to perform (e.g., `action.click` will contain `{"index": 10}`).
*   `for action_name, params in action.model_dump(exclude_unset=True).items()`: This loop iterates through the `ActionModel` to find which action was requested. `.model_dump(exclude_unset=True)` converts the `ActionModel` into a dictionary, showing only the fields that were actually set.
*   `if action_name.startswith("mcp")`: This is a placeholder for future ability to call actions exposed by *other* [MCP Servers (FastMCP)](01_mcp_server__fastmcp__.md). This means our agent can potentially interact with other MCP-enabled services using tools dynamically provided to it.
*   `await self.registry.execute_action(...)`: This is the core line. The `Controller`'s `registry` (which holds all its registered actions) is asked to execute the found `action_name` (e.g., "click") with the given `params` (e.g., `{"index": 10}`). Importantly, it passes the `browser_context` so the action knows which browser to perform the operation on.

The `Custom Controller` is the critical piece that bridges the AI's intelligent decisions ("what to do") with the actual browser movements ("how to do it"). It ensures that the AI's instructions are translated into precise and executable commands for the web browser.

## Conclusion

You've learned that the **Custom Controller (Action Registry)** is the mastermind behind executing specific browser actions. It acts as a central registry for all available browser commands, including custom ones like asking for human assistance or uploading files. When an AI agent decides on a browser action, the Custom Controller steps in to translate that instruction into concrete steps for the `Custom Browser`. It's the essential link that enables our AI agents to interact with the world wide web effectively.

In the next chapter, we'll shift our focus to the "brains" behind the operation, the AI models themselves, and learn about the [LLM Provider (Language Model Integration)](07_llm_provider__language_model_integration__.md).

[Next Chapter: LLM Provider (Language Model Integration)](07_llm_provider__language_model_integration__.md)

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="07_llm_provider__language_model_integration__.md">
# Chapter 7: LLM Provider (Language Model Integration)

In the [previous chapter](06_custom_controller__action_registry__.md), we saw how the `Custom Controller` translates AI's decisions into concrete browser actions, like clicking a button. But where do these "decisions" come from in the first place? And how does the AI understand complicated tasks like "find the definition of AI" or "research the job market impact of AI"?

This is where the **LLM Provider (Language Model Integration)** comes in! Think of it as the application's "brain" or "translator." It allows our system to talk to powerful Artificial Intelligence models, called LLMs (Large Language Models), which are very good at understanding human language, generating text, and making smart decisions.

**Central Use Case:** Our `Browser Use Agent` (from [Chapter 3](03_browser_use_agent__browser_automation_orchestrator__.md)) needs to decide its next step while browsing. It has a screenshot of a webpage and the task "find the search bar." The LLM Provider is how it asks a powerful AI model: "Given this screenshot and this task, what should I do next?" and gets a smart answer like "Type 'search term' into the element with index 5."

The LLM Provider is the component that makes this conversation possible, allowing our application to tap into the intelligence of large AI models.

## What is the LLM Provider?

The LLM Provider is essentially a "universal adapter" for different AI brains. Imagine you have a device that can plug into many different types of wall sockets (US, EU, UK, etc.). The LLM Provider is similar â€“ it lets our application connect to and use various Large Language Models (LLMs) from different companies (like Google Gemini, OpenAI's GPT, Anthropic Claude, etc.) without having to rewrite our code for each one.

Here's why it's so important:

*   **Intelligence Source:** It's the primary way our agents (like the [Browser Use Agent](03_browser_use_agent__browser_automation_orchestrator__.md) and [Deep Research Agent](04_deep_research_agent__.md)) get their "intelligence." They send questions or situations to the LLM, and the LLM responds with thoughts, actions, or generated text.
*   **Flexibility:** It means you can easily switch between different LLMs based on your preference, cost, or performance needs, just by changing a setting.
*   **Unified Interface:** It provides a consistent way for our application to "speak" to any LLM, no matter its underlying technology.

## How the LLM Provider Works: A Simple Scenario

Let's revisit our central use case: The `Browser Use Agent` needs to "find the search bar" on a webpage.

1.  **Agent Needs a Decision:** The `Browser Use Agent` (from [Chapter 3](03_browser_use_agent__browser_automation_orchestrator__.md)) reaches a point where it needs to decide what to do next. It has observed the browser's current state (e.g., screenshot, list of clickable elements).
2.  **Agent Prepares Question:** It puts together a question for the LLM, including:
    *   The overall task (e.g., "Find the search bar.")
    *   A description of the current webpage (e.g., "Here's a list of visible elements, including a 'search' button and an input field.")
    *   Information about the tools it can use (e.g., "I can `click` on elements, `type` text, or `extract_text`.")
3.  **Agent Requests LLM:** The `Browser Use Agent` sends this whole "context" to the LLM Provider.
4.  **LLM Provider Translates:** The LLM Provider takes this request and translates it into the specific format that the chosen LLM understands (e.g., for Google Gemini, it uses the Gemini API; for OpenAI, it uses the OpenAI API). It also adds your API key (from [Chapter 2](02_settings___configuration_.md)) to authenticate the request.
5.  **LLM Processes Request:** The LLM (e.g., Gemini) reads the context, "thinks" about the problem, and decides the best action.
6.  **LLM Provider Gets Response:** The LLM Provider receives the LLM's answer (e.g., "Thought: I should type into the search bar. Action: Type 'search term' into element with index 5").
7.  **LLM Provider Translates Back:** It translates the LLM's raw response into a structured format that our `Browser Use Agent` understands (like the `ActionModel` we saw in [Chapter 6](06_custom_controller__action_registry__.md)).
8.  **Agent Receives Decision:** The `Browser Use Agent` gets the structured decision and proceeds to execute the action using the `Custom Controller`.

## How to Configure the LLM Provider

The beautiful part of the LLM Provider is its reliance on our [Settings & Configuration](02_settings___configuration__.md). You don't write code to switch LLMs; you just change a setting in your `.env` file!

To use the LLM Provider, you need to:

1.  **Choose a Provider and Model:** Decide which LLM you want to use (e.g., `google`, `openai`, `anthropic`).
2.  **Provide an API Key:** Get an API key from your chosen LLM provider and store it securely.

Let's say you want to use Google's Gemini model.

**1. Update your `.env` file:**

Open your `.env` file (from [Chapter 2](02_settings___configuration__.md)) in the root of your project and add/modify these lines:

```dotenv
# .env file

# --- LLM Settings ---
MCP_LLM_PROVIDER=google
MCP_LLM_MODEL_NAME=gemini-1.5-flash-latest # Or another model from the list below
MCP_LLM_TEMPERATURE=0.0 # How "creative" the AI is (0.0 is more factual)

# Your API key for Google Gemini
MCP_LLM_API_KEY=YOUR_GOOGLE_GEMINI_API_KEY_HERE
# Or, if you prefer specific:
# MCP_LLM_GOOGLE_API_KEY=YOUR_GOOGLE_GEMINI_API_KEY_HERE
```
**Explanation:**

*   `MCP_LLM_PROVIDER=google`: This tells the `LLMSettings` (from `config.py`) that we want to use Google's LLMs.
*   `MCP_LLM_MODEL_NAME=gemini-1.5-flash-latest`: This specifies the exact Gemini model to use. You can find available models in `src/mcp_server_browser_use/_internal/utils/config.py` under the `model_names` dictionary for your chosen provider.
*   `MCP_LLM_TEMPERATURE=0.0`: A lower temperature makes the LLM's responses more consistent and less random, which is usually good for automation tasks.
*   `MCP_LLM_API_KEY=YOUR_GOOGLE_GEMINI_API_KEY_HERE`: **Crucially, replace `YOUR_GOOGLE_GEMINI_API_KEY_HERE` with your actual API key!** You get this from the LLM provider's website. For most providers, you can use the generic `MCP_LLM_API_KEY`, but for clarity or specific multi-LLM setups, you can use provider-specific keys like `MCP_LLM_GOOGLE_API_KEY`.

Now, when you run any command that involves an AI agent (like `run-browser-agent` or `run-deep-research`), the system will automatically use Google's Gemini model with your specified key!

You can easily switch to OpenAI by changing `MCP_LLM_PROVIDER=openai` and providing `MCP_LLM_OPENAI_API_KEY` (or `MCP_LLM_API_KEY`).

## Inside the LLM Provider: Under the Hood

The core logic for interacting with various LLMs sits in `src/mcp_server_browser_use/_internal/utils/llm_provider.py`. It uses a popular Python library called `langchain`, which is designed to make it easy to work with different LLMs.

### The Flow: A High-Level Diagram

```mermaid
sequenceDiagram
    participant Agent as Browser Use Agent (or Deep Research Agent)
    participant LP as LLM Provider
    participant LLM_API as Google/OpenAI/Anthropic API
    participant LLM_Model as Gemini/GPT/Claude Model

    Agent->>LP: "Get LLM model for specific settings" (e.g., config for Gemini)
    LP->>LP: Reads settings: provider="google", model="gemini-1.5-flash", api_key="..."
    LP->>LLM_API: Initializes connection to Google Gemini API with API Key
    LLM_API-->>LP: Ready to communicate
    LP-->>Agent: Returns a "ready-to-use" LLM object

    Agent->>LP: "Invoke LLM" with Request (e.g., "Given page, what's next?")
    LP->>LLM_API: Sends structured request to Gemini API
    LLM_API->>LLM_Model: Passes request to the actual AI model
    LLM_Model-->>LLM_API: Returns AI's response (thoughts, actions)
    LLM_API-->>LP: Passes raw response back
    LP->>LP: Formats raw response into our application's format
    LP-->>Agent: Returns structured LLM response
```
This diagram highlights that the `LLM Provider` acts as the middleman. First, it sets up the connection to the correct LLM based on our configuration. Then, it handles all communication with that LLM, making sure requests are sent in the right format and responses are understood by our agents.

### Core Components in Code

The `llm_provider.py` file contains functions that, based on your configuration, return the correct `langchain` object for interacting with a specific LLM.

Let's look at the main function, `get_llm_model`, which is responsible for creating the LLM object.

```python
# src/mcp_server_browser_use/_internal/utils/llm_provider.py (Simplified)

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from pydantic import SecretStr

def get_llm_model(
    provider: str,
    api_key: Optional[SecretStr], # This comes from our config.settings.get_api_key_for_provider()
    model_name: str,
    temperature: float,
    base_url: Optional[str] = None,
    **kwargs: Any,
) -> BaseLanguageModel:
    """
    Returns a configured LLM model instance based on the specified provider.
    """
    api_key_val = api_key.get_secret_value() if api_key else None

    if provider == "openai":
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            api_key=api_key_val,
            base_url=base_url,
        )
    elif provider == "anthropic":
        return ChatAnthropic(
            model=model_name,
            temperature=temperature,
            api_key=api_key_val,
            base_url=base_url,
        )
    elif provider == "google":
        return ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            api_key=api_key_val,
        )
    # ... (other providers like azure_openai, deepseek, ollama, etc.)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

```
**Explanation:**

*   `get_llm_model(...)`: This function takes the LLM settings (provider, model name, API key, temperature, etc.) that our `AppSettings` object (from [Chapter 2](02_settings___configuration__.md)) has loaded from your `.env` file.
*   `api_key.get_secret_value()`: API keys are stored as `SecretStr` for security, and this line safely retrieves their actual string value.
*   `if provider == "openai": return ChatOpenAI(...)`: If the provider is `"openai"`, it creates and returns a `ChatOpenAI` object from the `langchain_openai` library, passing in the model name, temperature, and API key.
*   `elif provider == "google": return ChatGoogleGenerativeAI(...)`: Similarly, for `"google"`, it returns a `ChatGoogleGenerativeAI` object.
*   This function handles all the different `langchain` classes and their specific setup requirements, so the rest of our application doesn't have to worry about the details.

### How Our Agents Use It

Recall from [Chapter 3](03_browser_use_agent__browser_automation_orchestrator__.md) that the `Browser Use Agent` used `self.llm.invoke(messages)` to get decisions from the LLM.

```python
# src/mcp_server_browser_use/_internal/agent/browser_use/browser_use_agent.py (Simplified)

# ... inside BrowserUseAgent's initialization ...
from mcp_server_browser_use.config import settings
from mcp_server_browser_use._internal.utils import llm_provider

# Get the main LLM configuration from global settings
llm_config = settings.get_llm_config(is_planner=False)
self.llm = llm_provider.get_llm_model(**llm_config)

# ... later, when the agent needs LLM's help ...
llm_output_str: str = await self.llm.invoke(messages)
```
**Explanation:**

*   `llm_config = settings.get_llm_config(is_planner=False)`: This retrieves all the LLM-related settings (provider, model, key, etc.) from our global `settings` object.
*   `self.llm = llm_provider.get_llm_model(**llm_config)`: This line calls the `get_llm_model` function we just discussed, which returns a ready-to-use LLM object (e.g., `ChatGoogleGenerativeAI`).
*   `await self.llm.invoke(messages)`: Once the LLM object is created and assigned to `self.llm`, the agent simply calls its `invoke` method, passing the messages. The Complexity of talking to different LLM APIs is hidden away by the `llm_provider` module and `langchain`.

This setup makes our system highly modular and flexible. The intelligence of our agents truly comes from these powerful Language Models, seamlessly integrated through the LLM Provider.

## Conclusion

You've learned that the **LLM Provider (Language Model Integration)** is the essential component that connects our `mcp-browser-use` application to powerful AI Language Models. It acts as a universal adapter, allowing us to easily switch between different LLMs just by changing a setting in our `.env` file. This flexibility and intelligence are crucial for our agents to understand complex tasks, plan actions, and generate responses.

This marks the end of our journey through the core components of the `mcp-browser-use` project. You now have a solid understanding of how all these partsâ€”from the central server to the LLM brainsâ€”work together to enable sophisticated browser automation and deep web research!

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="index.md">
# Tutorial: mcp-browser-use

This project (**mcp-browser-use**) is an *AI-driven browser automation server* that enables large language models (LLMs) to control a web browser and perform complex web research. It acts as a central hub, receiving instructions, automating browser actions, and generating structured reports, effectively turning AI into a powerful web assistant for tasks like *web browsing* and *deep information gathering*.


**Source Repository:** [https://github.com/Saik0s/mcp-browser-use.git](https://github.com/Saik0s/mcp-browser-use.git)

```mermaid
flowchart TD
    A0["Settings & Configuration
"]
    A1["MCP Server (FastMCP)
"]
    A2["Custom Browser (Playwright Wrapper)
"]
    A3["Browser Use Agent (Browser Automation Orchestrator)
"]
    A4["Deep Research Agent
"]
    A5["LLM Provider (Language Model Integration)
"]
    A6["Custom Controller (Action Registry)
"]
    A0 -- "Configures" --> A1
    A0 -- "Configures" --> A2
    A0 -- "Configures" --> A3
    A0 -- "Configures" --> A4
    A0 -- "Configures" --> A5
    A1 -- "Invokes" --> A3
    A1 -- "Invokes" --> A4
    A3 -- "Controls" --> A2
    A4 -- "Controls" --> A2
    A3 -- "Uses for decisions" --> A5
    A4 -- "Uses for planning" --> A5
    A3 -- "Executes actions via" --> A6
    A2 -- "Provides context to" --> A6
    A6 -- "Registers actions with" --> A1
```

## Chapters

1. [MCP Server (FastMCP)
](01_mcp_server__fastmcp__.md)
2. [Settings & Configuration
](02_settings___configuration_.md)
3. [Browser Use Agent (Browser Automation Orchestrator)
](03_browser_use_agent__browser_automation_orchestrator__.md)
4. [Deep Research Agent
](04_deep_research_agent_.md)
5. [Custom Browser (Playwright Wrapper)
](05_custom_browser__playwright_wrapper__.md)
6. [Custom Controller (Action Registry)
](06_custom_controller__action_registry__.md)
7. [LLM Provider (Language Model Integration)
](07_llm_provider__language_model_integration__.md)


---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

</files>
