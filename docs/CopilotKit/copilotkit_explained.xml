This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
01_actions__frontend___backend__.md
02_copilotkitprovider__react_component__.md
03_frontend_hooks____copilotkit_react_core___.md
04_ui_components____copilotkit_react_ui___.md
05_copilottextarea____copilotkit_react_textarea___.md
06_copilotruntime__backend_engine__.md
07_agents__backend_focus__.md
08_service_adapters__backend__.md
09_runtime_events___protocol_.md
index.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="01_actions__frontend___backend__.md">
# Chapter 1: Actions (Frontend & Backend)

Welcome to the CopilotKit tutorial! We're excited to help you build amazing AI-powered applications. In this first chapter, we'll dive into one of the most fundamental concepts: **Actions**.

Imagine you're building a smart assistant for a project management app. You want your users to be able to say things like, "Hey Copilot, create a new task called 'Design new logo' and assign it to Sarah." For the AI to actually *create that task*, it needs a specific instruction or a "tool" to do so. That's where Actions come in!

**What Problem Do Actions Solve?**

Large Language Models (LLMs) are fantastic at understanding language and generating text. However, on their own, they can't directly interact with your application's features or external services. They can't click buttons, save data to your database, or send emails.

**Actions bridge this gap.** They are specific tasks or capabilities you define that your AI copilot can perform. Think of them as giving your AI a set of tools. For our project management example, an action could be "createTask" or "assignUserToTask." The AI, based on the conversation, can then decide to use these tools to fulfill the user's request.

Actions can be defined in two main places:

1.  **Frontend (React):** These actions typically interact with your application's user interface (UI). For example, an action could update a list on the screen, display a confirmation pop-up, or fill out a form.
2.  **Backend (Python/Node.js):** These actions are for server-side operations, like saving data to a database, calling other APIs (e.g., sending an email via SendGrid), or performing complex calculations.

Let's see how this works with a very simple example: a to-do list application. We want our copilot to be able to add a new task to our list.

### Frontend Action: Adding a Task to the UI

In your React application, you might want an action that adds a task directly to the list displayed on the user's screen. You can define this using the `useCopilotAction` hook, which we'll learn more about in the [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md) chapter.

Here's a simplified example:

```tsx
// In your React component (e.g., TodoListComponent.tsx)
import { useCopilotAction } from "@copilotkit/react-core";
// import { useState } from "react"; // You'd use this for a real list

// Let's imagine you have a function to update your UI
// const [tasks, setTasks] = useState([]);

useCopilotAction({
  name: "addTaskToUI",
  description: "Adds a new task to the to-do list visible on the screen.",
  parameters: [ // Defines what information the action needs
    { name: "taskText", type: "string", description: "The content of the task." }
  ],
  handler: async ({ taskText }) => {
    // In a real app, you'd update your React state here:
    // setTasks(prevTasks => [...prevTasks, taskText]);
    alert(`Task added to UI: ${taskText}`);
    return `${taskText} has been added to your list on the screen.`;
  },
});
```

Let's break this down:
*   `name: "addTaskToUI"`: This is the unique identifier for your action. The AI will use this name to call it.
*   `description: "Adds a new task..."`: This helps the AI understand what the action does and when to use it. Good descriptions are key!
*   `parameters: [...]`: This tells the AI what pieces of information the action needs to run. Here, it needs a `taskText` which should be a `string`. The AI will try to extract this from the user's conversation.
*   `handler: async ({ taskText }) => { ... }`: This is the actual JavaScript function that gets executed when the AI decides to use this action. It receives the `taskText` (extracted by the AI) as an argument. In this example, it just shows an alert, but in a real app, it would update your React component's state to re-render the list with the new task.

**Input/Output:**
*   **User says:** "Copilot, add 'Buy groceries' to my list."
*   **AI understands:** It likely needs to use an action related to adding tasks. It sees `addTaskToUI` and its description.
*   **AI extracts parameters:** It identifies "Buy groceries" as the `taskText`.
*   **Action executes:** The `handler` function runs with `taskText` being "Buy groceries".
*   **Result:** An alert pops up saying "Task added to UI: Buy groceries", and the handler returns a message that the Copilot can use to respond to the user.

### Backend Action: Saving a Task to a Database

Sometimes, interacting with the UI isn't enough. You might want to save the task permanently in a database. This is where backend actions come in handy. Here's a conceptual Python example:

```python
# In your Python backend (e.g., actions_backend.py)
from copilotkit.action import Action # From sdk-python/copilotkit/action.py
from typing import List, Optional, Callable # For type hinting

# Dummy function to simulate saving to a database
async def save_task_to_database(task_text: str):
    print(f"Attempting to save to DB: {taskText}")
    # ... actual database saving logic would go here ...
    return f"Task '{task_text}' has been saved to the database."

# Define the action
saveTaskAction = Action(
    name="saveTaskPersistent",
    description="Saves a new task to the persistent storage (database).",
    parameters=[ # These are Parameter objects
      {"name": "task_text", "type": "string", "description": "The task to save."}
    ],
    handler=save_task_to_database
)
```

Breaking it down:
*   `Action(...)`: This is how you define an action in the Python SDK.
*   `name: "saveTaskPersistent"`: Again, a unique name.
*   `description: "Saves a new task..."`: Helps the AI differentiate this from the UI-only action.
*   `parameters: [...]`: Same idea as the frontend, defining the inputs. The structure might look slightly different based on the SDK's requirements (e.g., a list of dictionaries).
*   `handler=save_task_to_database`: The function (`save_task_to_database`) that will be called. This function would contain your Python logic to interact with a database.

**Input/Output:**
*   **User says:** "Hey Copilot, make sure 'Book flight tickets' is saved."
*   **AI understands:** The keyword "saved" might make it prefer the `saveTaskPersistent` action.
*   **AI extracts parameters:** It identifies "Book flight tickets" as `task_text`.
*   **Action executes:** The `save_task_to_database` Python function runs.
*   **Result:** The Python function prints to the server console and returns a success message like "Task 'Book flight tickets' has been saved to the database." The AI can then relay this confirmation to the user.

### How It Works Under the Hood (A Simplified View)

When you use CopilotKit, you're setting up a system where your frontend and backend can communicate efficiently with an AI model.

1.  **User Interaction:** The user types a message into a chat interface (likely one of the [UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui__.md)).
2.  **Message to Backend:** This message, along with information about available actions (both frontend and backend ones), is sent to the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md). The `CopilotKitProvider` (covered in [CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)) helps manage this.
3.  **AI Processing:** The backend runtime forwards the request to an LLM (like OpenAI's GPT). The LLM analyzes the user's intent and looks at the descriptions of all available actions.
4.  **Action Decision:** The LLM decides if an action should be performed. If so, it specifies which action and what arguments to use.
5.  **Execution:**
    *   **Backend Action:** If it's a backend action (like `saveTaskPersistent`), the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) executes the action's handler function directly on the server.
    *   **Frontend Action:** If it's a frontend action (like `addTaskToUI`), the backend runtime sends a command back to the frontend. The frontend, using hooks like `useCopilotAction` from [`@copilotkit/react-core`](03_frontend_hooks____copilotkit_react_core___.md), then executes the appropriate handler.
6.  **Result & Response:** The result of the action (e.g., a success message or data) is sent back to the LLM via the backend. The LLM can then use this result to formulate a natural language response to the user.

Here's a diagram illustrating the flow:

```mermaid
sequenceDiagram
    participant User
    participant FrontendUI as Frontend UI (React)
    participant CopilotKitCore as CopilotKit React Core
    participant BackendRuntime as CopilotRuntime (Backend)
    participant LLM

    User->>FrontendUI: "Add 'Write report' to my UI list"
    FrontendUI->>BackendRuntime: Sends user message & available actions list
    BackendRuntime->>LLM: Processes message + actions context
    LLM->>BackendRuntime: Decides to call "addTaskToUI" with "Write report"
    BackendRuntime->>CopilotKitCore: Instructs frontend to execute "addTaskToUI"
    CopilotKitCore->>CopilotKitCore: Executes `handler` for "addTaskToUI" (e.g., shows alert)
    CopilotKitCore-->>BackendRuntime: Returns result (e.g., "Task added to UI")
    BackendRuntime-->>LLM: Forwards action result
    LLM->>BackendRuntime: Generates final user-facing response
    BackendRuntime->>FrontendUI: Sends final response
    FrontendUI->>User: Displays: "Okay, I've added 'Write report' to your list on the screen."
```

### Diving a Bit Deeper into the Code

**Frontend Action Registration (`useCopilotAction`)**

When you use `useCopilotAction` in your React component, you're essentially telling CopilotKit, "Hey, here's a capability my frontend can perform."

```typescript
// Simplified concept from CopilotKit/packages/react-core/src/hooks/use-copilot-action.ts
function useCopilotAction(action: FrontendAction) {
  // CopilotContext is provided by CopilotKitProvider
  const { setAction } = useCopilotContext();
  const uniqueActionId = "some_unique_id_for_this_action_instance"; // Generated internally

  // When the component mounts or action definition changes:
  useEffect(() => {
    setAction(uniqueActionId, action); // Register the action
    // ... cleanup to removeAction when component unmounts ...
  }, [action.name /*, other dependencies like handler, description */]);
}
```
This `setAction` function, part of the `CopilotContext` (which we'll explore with the [CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)), makes the action known to the CopilotKit system running in the browser. This list of frontend actions is then communicated to the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md).

**Backend Action Definition (Python SDK)**

In Python, the `Action` class is a straightforward way to structure your backend capabilities.

```python
# Simplified from sdk-python/copilotkit/action.py
class Action:
    def __init__(
            self,
            name: str,
            handler: Callable, # The function to execute
            description: Optional[str] = None,
            parameters: Optional[List[Parameter]] = None, # Expected inputs
        ):
        self.name = name
        self.description = description
        self.parameters = parameters
        self.handler = handler
        # ... some validation for the name ...
```
This class simply holds all the necessary information about an action: its name, what it does (`description`), what inputs it needs (`parameters`), and the function to call (`handler`). Your [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) will maintain a list of these `Action` objects.

**Executing a Backend Action**

When the LLM decides to use a backend action, the Python runtime will find the corresponding `Action` object and invoke its `execute` method (or directly call the handler).

```python
# Simplified concept from sdk-python/copilotkit/action.py
# class Action:
    async def execute(self, arguments: dict) -> ActionResultDict:
        # The handler is the function you provided, like 'save_task_to_database'
        result = self.handler(**arguments) # Call your function with extracted args

        # Handle if your handler is an async function
        if iscoroutinefunction(self.handler):
            actual_result = await result
        else:
            actual_result = result
        return {"result": actual_result}
```

**Action Schemas: `FrontendAction` and `ActionDict`**

For the AI to understand what an action does and what data it needs, actions must be described in a structured way.
In the frontend (TypeScript), this is often represented by types like `FrontendAction`:

```typescript
// Snippet from CopilotKit/packages/react-core/src/types/frontend-action.ts
export type FrontendAction<T extends Parameter[] | [] = []> = Action<T> & {
  name: string; // e.g., "addTaskToUI"
  description?: string; // Explains what it does
  parameters?: T; // Defines expected arguments like { name: "taskText", type: "string" }
  handler?: (args: MappedParameterTypes<T>) => any | Promise<any>;
  // ... other properties like 'render' for UI customization ...
};
```

In Python, an action can be represented as a dictionary (`ActionDict`) for communication or when being prepared for the LLM:

```python
# From sdk-python/copilotkit/action.py
class ActionDict(TypedDict):
    name: str
    description: str
    parameters: List[Parameter] # Parameter is another TypedDict describing an argument
```

These structures (often converted into a format like JSON Schema, as seen in `processActionsForRuntimeRequest` in `frontend-action.ts` or `actionParametersToJsonSchema` in `@copilotkit/shared`) are crucial. They allow the LLM to "see" the tools you've provided, understand their purpose, and know how to use them by providing the correct arguments. The communication details are part of the [Runtime Events & Protocol](09_runtime_events___protocol_.md).

### Conclusion

You've just taken your first step into empowering your AI copilot! Actions are the bridge between the AI's understanding and your application's capabilities. By defining actions, you give your AI the "tools" it needs to perform useful tasks, whether it's updating the UI on the frontend or executing logic on the backend.

Key takeaways:
*   Actions define specific tasks your AI can perform (e.g., `sendEmail`, `updateSpreadsheet`).
*   They can be frontend-based (React, for UI interactions) or backend-based (Python/Node.js, for server logic).
*   Each action has a `name`, `description`, `parameters` (inputs it needs), and a `handler` (the function that does the work).
*   CopilotKit uses these definitions to allow the AI to intelligently choose and execute your custom functions based on the user's conversation.

Now that you have a grasp of what Actions are, let's move on to setting up the foundational piece for your CopilotKit application in React: the [CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md). This provider is essential for making everything work together.

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="02_copilotkitprovider__react_component__.md">
# Chapter 2: CopilotKitProvider (React Component)

In [Chapter 1: Actions (Frontend & Backend)](01_actions__frontend___backend__.md), we learned how to define specific tasks ("Actions") that our AI copilot can perform. These actions are like giving our AI a toolbox. But how do all these tools, and the AI itself, get connected and powered up within our React application? That's where the `CopilotKitProvider` comes in!

**What's the Big Idea? The Central Power Hub**

Imagine you're building a house (your React app) and you want to install a bunch of smart, AI-powered appliances (like an AI assistant in your chat, or AI that can automatically fill forms). These appliances need electricity and a way to talk to each other and to the main control system.

The `CopilotKitProvider` is like the main electrical panel and wiring system for all CopilotKit features in your app. You wrap it around your application, or the parts of your app that need AI capabilities.

**What Problem Does It Solve?**

Without `CopilotKitProvider`, other CopilotKit components (like a chat window) or hooks (like `useCopilotAction` we saw in Chapter 1) wouldn't work. They'd be like unplugged appliances. The provider supplies:

1.  **Configuration:** It tells other CopilotKit parts where your backend AI engine ([CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md)) is located.
2.  **Context:** It shares important information and functions. For example, when you define a frontend action using `useCopilotAction`, that action needs to register itself with a central system. The `CopilotKitProvider` provides this system.
3.  **Communication Lines:** It facilitates the communication between your frontend AI components and your backend AI engine.

Essentially, it's the foundational piece that "activates" CopilotKit in your React application.

**How to Use `CopilotKitProvider`**

Using it is quite straightforward. You import `CopilotKit` (which is the component providing the context, often aliased or re-exported as `CopilotKitProvider` in spirit, though the actual component name used in examples is `CopilotKit`) from `@copilotkit/react-core` and wrap it around the parts of your application that will use AI features.

Most commonly, you'll wrap your entire application.

Let's look at a very basic example:

```tsx
// App.tsx (or your main application file)
import React from "react";
import { CopilotKit } from "@copilotkit/react-core";
import { CopilotSidebar } from "@copilotkit/react-ui"; // A UI component we'll see later
import "./App.css"; // Your app's styles

function MyAwesomeApp() {
  return (
    <div>
      <h1>My AI-Powered App</h1>
      {/* Other app content can go here */}
    </div>
  );
}

function App() {
  return (
    <CopilotKit runtimeUrl="/api/copilotkit"> {/* Power ON! */}
      <MyAwesomeApp />
      <CopilotSidebar /> {/* This chat UI needs the Provider */}
    </CopilotKit>
  );
}

export default App;
```

Let's break this down:
*   `import { CopilotKit } from "@copilotkit/react-core";`: We import the necessary component.
*   `<CopilotKit runtimeUrl="/api/copilotkit">`: This is the magic part!
    *   `CopilotKit`: This is the provider component.
    *   `runtimeUrl="/api/copilotkit"`: This is a crucial prop. It tells `CopilotKit` where your backend AI logic (the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md)) is running. This URL is the endpoint that your frontend will communicate with for AI tasks. We'll cover setting up this backend endpoint in a later chapter.
*   `<MyAwesomeApp />` and `<CopilotSidebar />`: Any components placed *inside* `CopilotKit` (like `MyAwesomeApp` or the `CopilotSidebar` UI component) can now use CopilotKit features. The `CopilotSidebar` is a pre-built chat interface from [`@copilotkit/react-ui`](04_ui_components____copilotkit_react_ui___.md) that needs the provider to function.

**What Happens When You Use It?**

When you add `<CopilotKit ...>`, it does a few important things:
1.  **Sets up a "Copilot Context":** Think of this as a special backpack of information that all child components can access. This backpack contains the `runtimeUrl`, functions to manage actions, and other necessary settings.
2.  **Initializes Communication:** It prepares to talk to your backend AI service specified by `runtimeUrl`.
3.  **Enables Hooks and Components:** Hooks like `useCopilotAction` (from [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)) and UI components like `CopilotSidebar` (from [UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md)) can now "reach into" this context to get what they need.

If you tried to use `CopilotSidebar` or `useCopilotAction` *outside* of a `CopilotKitProvider`, your app would likely throw an error because they wouldn't find the necessary context. It's like trying to turn on a lamp that isn't plugged into a power outlet.

**Key Props for `CopilotKitProvider`**

While `runtimeUrl` is the most fundamental, here are a few other important props you might encounter (from `CopilotKit/packages/react-core/src/components/copilot-provider/copilotkit-props.tsx`):

*   `publicApiKey`: If you're using CopilotKit Cloud services, you'll provide your API key here.
*   `headers`: Allows you to send custom HTTP headers with requests to your backend, useful for authentication.
    ```tsx
    <CopilotKit
      runtimeUrl="/api/copilotkit"
      headers={{
        'Authorization': 'Bearer YOUR_AUTH_TOKEN'
      }}
    >
      {/* ...your app... */}
    </CopilotKit>
    ```
*   `children`: This is a standard React prop representing the components nested inside the provider.

**Under the Hood: A Peek into the Mechanism**

So, how does `CopilotKitProvider` make all this information available? It uses a core React concept called **Context**.

1.  **React Context API:** React's Context API provides a way to pass data through the component tree without having to pass props down manually at every level. It's perfect for global data like themes, user information, or in our case, CopilotKit's configuration and core functionalities.

2.  **`CopilotContext`:** `CopilotKit` creates a `CopilotContext`. This context object holds all the shared state and functions.
    (Simplified from `CopilotKit/packages/react-core/src/context/copilot-context.tsx`)
    ```typescript
    // Simplified concept of what the context might hold
    interface CopilotContextParams {
      copilotApiConfig: { chatApiEndpoint: string; /* ...other configs */ };
      actions: Record<string, FrontendAction<any>>;
      setAction: (id: string, action: FrontendAction<any>) => void;
      // ... many other useful things!
    }

    // Creates the actual context with a default empty value
    const CopilotContext = React.createContext<CopilotContextParams>(/* default value */);
    ```
    This `CopilotContext` is defined with all the properties and functions that CopilotKit components and hooks will need.

3.  **`<CopilotContext.Provider>`:** The `CopilotKit` component (from `CopilotKit/packages/react-core/src/components/copilot-provider/copilotkit.tsx`) internally uses `<CopilotContext.Provider>` to make the context value available to all its descendant components.
    ```tsx
    // Simplified inside CopilotKitInternal component
    function CopilotKitInternal(props: CopilotKitProps) {
      // ... lots of state and setup logic ...
      const [actions, setActions] = useState({});
      const copilotApiConfig = { chatApiEndpoint: props.runtimeUrl, /* ... */ };

      const contextValue = {
        copilotApiConfig,
        actions,
        setAction: (id, action) => { /* ... logic to add action ... */ },
        // ... and all other context values
      };

      return (
        <CopilotContext.Provider value={contextValue}>
          {props.children}
        </CopilotContext.Provider>
      );
    }
    ```
    When you write `<CopilotKit runtimeUrl="...">`, it's essentially setting up this provider with the `runtimeUrl` and other configurations.

4.  **Accessing the Context:** Other CopilotKit hooks (like `useCopilotAction` or the internal `useCopilotContext` hook) and components then use React's `useContext` hook to access the data provided by `CopilotKitProvider`.
    ```typescript
    // Simplified from CopilotKit/packages/react-core/src/context/copilot-context.tsx
    function useCopilotContext(): CopilotContextParams {
      const context = React.useContext(CopilotContext);
      if (context === /* default empty value */) {
        // This error is a helpful reminder!
        throw new Error("Remember to wrap your app in a `<CopilotKit> ... </CopilotKit>` !!!");
      }
      return context;
    }
    ```
    If a component tries to use `useCopilotContext` without a `CopilotKit` ancestor, it gets the default empty value, triggering the helpful error message.

Here's a visual representation of how it works:

```mermaid
sequenceDiagram
    participant YourApp as Your React App
    participant CKitProvider as CopilotKitProvider (<CopilotKit>)
    participant ReactContextAPI as React Context
    participant CopilotHook as e.g., useCopilotAction
    participant CopilotUI as e.g., CopilotSidebar

    YourApp->>CKitProvider: Renders <CopilotKit runtimeUrl="...">
    CKitProvider->>ReactContextAPI: Creates CopilotContext & provides value
    Note over CKitProvider, ReactContextAPI: Context now holds runtimeUrl, action registry, etc.

    YourApp->>CopilotHook: Component using useCopilotAction mounts
    CopilotHook->>ReactContextAPI: Calls useContext(CopilotContext)
    ReactContextAPI-->>CopilotHook: Returns context value
    CopilotHook->>CopilotHook: Uses context (e.g., to register an action)

    YourApp->>CopilotUI: CopilotSidebar component mounts
    CopilotUI->>ReactContextAPI: Calls useContext(CopilotContext)
    ReactContextAPI-->>CopilotUI: Returns context value
    CopilotUI->>CopilotUI: Uses context (e.g., to know backend URL for sending messages)
```

The `CopilotKitProvider` effectively acts as the root of the CopilotKit system within your React component tree, initializing and making available all necessary configurations and functionalities to its children. The actual component you use is `<CopilotKit>`, as seen in the `CopilotKit/examples/next-openai/src/app/page.tsx` example. This component sets up the context using `CopilotContext.Provider` (visible in `CopilotKit/packages/react-core/src/components/copilot-provider/copilotkit.tsx`).

**Recap and What's Next**

The `CopilotKitProvider` (used as `<CopilotKit>`) is the heart of your CopilotKit integration in React. It's the main switch that powers everything up.

*   **It's Essential:** You *must* wrap your AI-enabled components with it.
*   **Provides Context:** It uses React's Context API to share configuration (like `runtimeUrl`) and core functions with other CopilotKit parts.
*   **Enables Communication:** It's the bridge allowing frontend components to interact with the backend AI engine.

Without it, your AI features simply won't have the power or information they need to operate.

Now that we understand how to "power on" CopilotKit in our app, we're ready to explore the specific tools that interact with this provider. Next, we'll dive into [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md), which allow us to define frontend actions and manage chat state.

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="03_frontend_hooks____copilotkit_react_core___.md">
# Chapter 3: Frontend Hooks (`@copilotkit/react-core`)

In [Chapter 2: CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md), we learned how to set up the `CopilotKitProvider` (used as `<CopilotKit>`), which is like the central power station for CopilotKit features in our React app. Now that our app is "powered up," let's explore some special tools that let us directly wire AI capabilities into our user interface: **Frontend Hooks** from the `@copilotkit/react-core` package.

**What's the Big Idea? Connecting Your UI to AI Magic**

Imagine your web application is a house. The `CopilotKitProvider` is the main electrical system. Frontend Hooks are like specialized outlets and switches you install in different rooms (your React components) to control AI-powered appliances or let those appliances understand what's happening in the room.

These hooks are special JavaScript functions (specifically, React Hooks) that you use in your frontend React components. They are the primary way you'll make your application's UI interactive with the AI.

**What Problem Do They Solve?**

Frontend Hooks provide easy ways to:

1.  **Tell the AI what's happening in your app:** For example, if you have a list of tasks, you can use a hook to make this list "readable" by the AI.
2.  **Let the AI control parts of your app:** You can define "actions" that the AI can trigger, like adding an item to that list directly within the UI.
3.  **Build custom AI chat experiences:** If you don't want to use pre-built chat windows, you can use hooks to manage chat messages and interactions yourself.
4.  **Communicate with more advanced backend AI agents:** For sophisticated AI behaviors, hooks can help your frontend talk to these agents.

Essentially, they act as the bridge between your app's user interface and CopilotKit's AI capabilities.

Let's meet the main hooks:

*   `useCopilotChat`: Lets you build custom chat UIs or directly interact with the chat's state (messages, loading status, etc.).
*   `useCopilotReadable`: Makes your application's data (like text, lists, or state variables) available to the AI as context. This helps the AI understand the current state of your app.
*   `useCopilotAction`: Allows you to define client-side functions (actions) that the AI can decide to call. For example, "add a new item to the shopping cart UI."
*   `useCoAgent`: Used for more advanced scenarios where you want to interact with AI agents running on the backend.

Let's see how these work with a simple to-do list example. Imagine we want our AI to help manage a to-do list displayed on our webpage.

### 1. `useCopilotAction`: Letting the AI Perform UI Tasks

This hook lets you define actions the AI can trigger in your frontend. In [Chapter 1: Actions (Frontend & Backend)](01_actions__frontend___backend__.md), we saw a conceptual example. Let's make it a bit more concrete.

Suppose you have a React component displaying a list of tasks, and you want the AI to be able to add a new task to this list.

```tsx
// In your TodoListComponent.tsx
import React, { useState } } from "react";
import { useCopilotAction } from "@copilotkit/react-core";

function TodoList() {
  const [tasks, setTasks] = useState<string[]>(["Buy milk"]);

  useCopilotAction({
    name: "addTaskToUI",
    description: "Adds a new task to the to-do list on the screen.",
    parameters: [{ name: "taskText", type: "string", description: "The task." }],
    handler: async ({ taskText }) => {
      setTasks(prevTasks => [...prevTasks, taskText]);
      alert(`Task added: ${taskText}`); // Simple feedback
    },
  });

  return (
    <div>
      <h2>My Todos</h2>
      <ul>{tasks.map(task => <li key={task}>{task}</li>)}</ul>
    </div>
  );
}
```

Let's break it down:
*   `useState(["Buy milk"])`: We have a simple state to hold our tasks.
*   `useCopilotAction({...})`: We define an action.
    *   `name: "addTaskToUI"`: The AI will use this name to call the action.
    *   `description`: Helps the AI understand what this action does.
    *   `parameters`: Tells the AI it needs a `taskText` (string).
    *   `handler`: The function that runs when the AI calls this action. Here, it updates our `tasks` state using `setTasks`, causing React to re-render the list with the new task. It also shows an alert.

**Input/Output:**
*   **User types into a Copilot chat (e.g., using `CopilotSidebar`):** "Add 'Finish report' to my to-do list."
*   **AI processes:** The AI understands the intent and sees the `addTaskToUI` action is available.
*   **Action executed:** The `handler` function runs with `taskText` being "Finish report".
*   **Result:** The `tasks` state in your `TodoList` component updates, the list on the screen shows "Finish report", and an alert pops up.

### 2. `useCopilotReadable`: Making Your App's Data Visible to the AI

What if you want the AI to know what tasks are *already* on your list? For example, if the user asks, "What's on my to-do list?" or "Is 'Buy milk' on my list?"

The `useCopilotReadable` hook makes parts of your application's state "readable" by the AI.

```tsx
// In your TodoListComponent.tsx (continuing from above)
import React, { useState } } from "react";
import { useCopilotAction, useCopilotReadable } from "@copilotkit/react-core";

function TodoList() {
  const [tasks, setTasks] = useState<string[]>(["Buy milk", "Call John"]);

  // ... (useCopilotAction for addTaskToUI remains the same) ...
  useCopilotAction({ /* ... as before ... */ });

  useCopilotReadable({
    description: "The current list of to-do items visible to the user.",
    value: tasks, // We provide the current tasks array
  });

  return (
    <div>
      <h2>My Todos</h2>
      <ul>{tasks.map(task => <li key={task}>{task}</li>)}</ul>
    </div>
  );
}
```
Here:
*   `useCopilotReadable({...})`:
    *   `description`: Explains to the AI what this data represents.
    *   `value: tasks`: We pass the `tasks` array (our component's state) to the AI. CopilotKit will automatically stringify this (e.g., `["Buy milk", "Call John"]`) and include it in the context sent to the LLM.

**Input/Output:**
*   **User asks:** "What tasks do I have?"
*   **AI processes:** The AI now has access to the `tasks` array (e.g., `["Buy milk", "Call John"]`) because of `useCopilotReadable`.
*   **AI responds:** "You have 'Buy milk' and 'Call John' on your list."

This hook can be used for any data you want the AI to be aware of – text content, form values, settings, etc.

### 3. `useCopilotChat`: Building Your Own Chat Interface

CopilotKit provides ready-to-use UI components like `CopilotSidebar` (which we'll see in [Chapter 4: UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md)). But what if you want to build a completely custom chat interface, or just programmatically send messages or read the chat history? `useCopilotChat` is your tool.

```tsx
// In a custom chat component, e.g., MyCustomChat.tsx
import React, { useState } from "react";
import { useCopilotChat } from "@copilotkit/react-core";
import { TextMessage, Role } from "@copilotkit/runtime-client-gql"; // For message types

function MyCustomChatInput() {
  const [inputText, setInputText] = useState("");
  const { appendMessage, visibleMessages, isLoading } = useCopilotChat();

  const handleSubmit = () => {
    if (!inputText.trim()) return;
    // Create a user message object
    const userMessage = new TextMessage({ content: inputText, role: Role.User });
    appendMessage(userMessage); // Send the message to the AI
    setInputText("");
  };

  return (
    <div>
      {/* You would display visibleMessages here */}
      <input value={inputText} onChange={(e) => setInputText(e.target.value)} />
      <button onClick={handleSubmit} disabled={isLoading}>
        {isLoading ? "Thinking..." : "Send"}
      </button>
    </div>
  );
}
```
Let's break this down:
*   `useCopilotChat()`: This hook gives you several useful things:
    *   `appendMessage`: A function to send a new message to the AI (and trigger a response).
    *   `visibleMessages`: An array of current chat messages (user, AI, system).
    *   `isLoading`: A boolean indicating if the AI is currently processing a response.
    *   ...and other utilities like `stopGeneration`, `reloadMessages`, etc.
*   `handleSubmit`: When the user clicks "Send", we create a `TextMessage` and use `appendMessage` to send it. CopilotKit then handles sending this to the backend AI, getting a response, and updating `visibleMessages`.

This hook gives you full control over the chat experience if you choose to build it from scratch.

### 4. `useCoAgent`: Interacting with Backend Agents

Sometimes, you'll have more complex AI logic defined as "agents" on your backend (we'll touch on these in [Chapter 7: Agents (Backend Focus)](07_agents__backend_focus__.md)). The `useCoAgent` hook is designed to help your frontend UI interact with these backend agents, often to display or manipulate shared state that the agent is working on.

For example, an AI agent might be building a complex document collaboratively with the user. `useCoAgent` could be used to:
*   Display the current state of the document part the agent is focused on.
*   Allow the user to make changes that the agent then incorporates.

```tsx
// Highly simplified concept for illustration
import { useCoAgent } from "@copilotkit/react-core";

function MyAgentDrivenComponent() {
  // Assume 'documentEditorAgent' is a defined backend agent
  const { state, setState } = useCoAgent<{ content: string }>({
    name: "documentEditorAgent",
    initialState: { content: "Start writing..." },
  });

  return (
    <textarea
      value={state.content}
      onChange={(e) => setState({ content: e.target.value })}
    />
  );
}
```
In this conceptual snippet:
*   `useCoAgent` connects to a backend agent named "documentEditorAgent".
*   It provides the current `state` from the agent (e.g., `{ content: "..." }`).
*   It provides a `setState` function. When the user types in the textarea, `setState` updates the local UI and also informs the backend agent about the change. The agent might then react to this new content.

`useCoAgent` facilitates a more dynamic, shared-state interaction between your UI and backend AI agents. Its usage is more advanced and often tied to how you structure your backend agents with tools like LangGraph.

**How Do These Hooks Work? Under the Hood**

Remember the `CopilotKitProvider` (from `<CopilotKit>`) we discussed in [Chapter 2: CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)? It creates a shared "Copilot Context" for your application. These frontend hooks tap into that context.

1.  **Registration (for `useCopilotAction` and `useCopilotReadable`):**
    *   When `useCopilotAction` is called in your component, it tells the `CopilotContext`: "Hey, here's a frontend action named 'X' with this handler function."
    *   When `useCopilotReadable` is called, it tells the `CopilotContext`: "Here's some data labeled 'Y', its current value is Z."
    *   The `CopilotContext` keeps track of all registered actions and readable context. This information is periodically sent to the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) so the AI knows what frontend capabilities and information are available.

2.  **Execution (for `useCopilotAction`):**
    *   User sends a message.
    *   The backend AI decides to use a frontend action (e.g., `addTaskToUI`).
    *   The backend runtime sends a command back to the frontend: "Execute 'addTaskToUI' with these arguments."
    *   The `CopilotContext` on the frontend receives this command, finds the registered `handler` for `addTaskToUI`, and executes it.

3.  **Data Flow (for `useCopilotChat`):**
    *   `useCopilotChat` also uses the `CopilotContext` to know the `runtimeUrl` (your backend AI endpoint).
    *   When you call `appendMessage`, the message is sent to this backend endpoint.
    *   The hook manages the state of messages, loading indicators, etc., based on the communication with the backend.

Here's a simplified diagram showing `useCopilotAction` at play:

```mermaid
sequenceDiagram
    participant User
    participant ReactComponent as React Component (e.g., TodoList)
    participant CopilotContext as CopilotKit React Context
    participant BackendRuntime as CopilotRuntime (Backend)
    participant LLM

    ReactComponent->>CopilotContext: `useCopilotAction("addTaskToUI", ...)` registers action
    Note over ReactComponent, CopilotContext: Action `addTaskToUI` is now known

    User->>ReactComponent: Types "Add 'New todo'" into chat UI
    ReactComponent->>BackendRuntime: Sends user message + list of known actions (including `addTaskToUI`)
    BackendRuntime->>LLM: Forwards message and actions for decision
    LLM->>BackendRuntime: Decides to call "addTaskToUI" with "New todo"
    BackendRuntime->>CopilotContext: Instructs frontend: "Execute addTaskToUI('New todo')"
    CopilotContext->>ReactComponent: Finds and calls the `handler` for "addTaskToUI"
    ReactComponent->>ReactComponent: `setTasks(...)` updates UI
    ReactComponent-->>CopilotContext: Action completes (maybe returns a status)
    CopilotContext-->>BackendRuntime: Action result/status
    BackendRuntime-->>LLM: Forwards result
    LLM->>BackendRuntime: Generates final response for user
    BackendRuntime->>ReactComponent: Sends final AI response to UI
    ReactComponent->>User: Displays AI confirmation
```

**Diving a Bit Deeper into the Code:**

*   **`useCopilotAction`**:
    (Simplified from `CopilotKit/packages/react-core/src/hooks/use-copilot-action.ts`)
    ```typescript
    // Simplified concept
    export function useCopilotAction<T extends Parameter[] | [] = []>(
      action: FrontendAction<T>,
      // ... dependencies
    ): void {
      const { setAction, removeAction } = useCopilotContext(); // Get functions from context
      const idRef = useRef<string>(/* some unique ID */);

      useEffect(() => {
        setAction(idRef.current, action as any); // Register the action with the context
        return () => {
          removeAction(idRef.current); // Clean up when component unmounts
        };
      }, [/* dependencies like action.name, action.handler */]);
    }
    ```
    This hook essentially uses `useEffect` to register (and unregister) the action definition with the central `CopilotContext` provided by `<CopilotKit>`.

*   **`useCopilotReadable`**:
    (Simplified from `CopilotKit/packages/react-core/src/hooks/use-copilot-readable.ts`)
    ```typescript
    // Simplified concept
    export function useCopilotReadable(
      options: UseCopilotReadableOptions,
      // ... dependencies
    ): string | undefined {
      const { addContext, removeContext } = useCopilotContext(); // Get functions from context
      const idRef = useRef<string>();

      useEffect(() => {
        const id = addContext(options.value, options.parentId, options.categories);
        idRef.current = id;
        return () => {
          removeContext(id); // Clean up
        };
      }, [/* dependencies like options.value */]);
      return idRef.current;
    }
    ```
    Similar to `useCopilotAction`, this hook registers the readable data with the `CopilotContext`. The context is then responsible for gathering all such readable pieces when constructing the information payload for the AI.

*   **`useCopilotChat`**:
    (Simplified from `CopilotKit/packages/react-core/src/hooks/use-copilot-chat.ts`)
    This hook is more complex as it wraps the `useChat` hook (an internal helper).
    ```typescript
    // Simplified concept
    export function useCopilotChat(options: UseCopilotChatOptions = {}): UseCopilotChatReturn {
      const context = useCopilotContext(); // Access runtimeUrl, actions, etc.
      const { messages, setMessages } = useCopilotMessagesContext(); // Access shared messages

      // ... lots of setup ...

      const { append, reload, stop } = useChat({ // Internal hook for chat logic
        ...options,
        actions: Object.values(context.actions), // Pass known actions to chat
        copilotConfig: context.copilotApiConfig, // Backend URL etc.
        messages,
        setMessages,
        // ... other callbacks and configurations ...
      });

      return {
        visibleMessages: messages,
        appendMessage: append, // Expose append function
        // ... other chat utilities ...
        isLoading: context.isLoading,
      };
    }
    ```
    `useCopilotChat` leverages the shared `CopilotContext` for configuration (like the backend URL and available actions) and another context (`CopilotMessagesContext`) for managing the actual list of messages. It then uses an internal `useChat` hook to handle the core logic of sending/receiving messages and interacting with the AI.

**Conclusion**

Frontend Hooks like `useCopilotAction`, `useCopilotReadable`, `useCopilotChat`, and `useCoAgent` are your workhorses for deeply integrating AI into your React application's UI. They provide the crucial connections between what the user sees and does on the frontend, and the AI's understanding and capabilities.

*   `useCopilotAction` gives your AI hands to interact with the UI.
*   `useCopilotReadable` gives your AI eyes to see the state of your UI.
*   `useCopilotChat` gives you the tools to build custom chat experiences.
*   `useCoAgent` bridges the gap to more complex backend AI agents.

By mastering these hooks, you can create rich, interactive, and intelligent user experiences.

Now that we've seen how to define actions and make data readable programmatically, what about pre-built UI elements to get a chat interface up and running quickly? That's what we'll explore in the next chapter: [UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="04_ui_components____copilotkit_react_ui___.md">
# Chapter 4: UI Components (`@copilotkit/react-ui`)

In [Chapter 3: Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md), we explored powerful hooks like `useCopilotAction` and `useCopilotChat`. These hooks give you fine-grained control to weave AI capabilities directly into your React components. But what if you want to add a standard AI chat interface to your app quickly, without building all the UI from scratch?

That's where the **UI Components** from the `@copilotkit/react-ui` package shine!

**What's the Big Idea? Ready-Made AI Interfaces**

Imagine you're building an e-commerce website. You've set up your AI with actions to find products and answer questions. Now, you need a chat window where users can talk to this AI. Instead of painstakingly building a chat box, message list, and input field yourself, you can just drop in a pre-built component from CopilotKit.

**What Problem Do They Solve?**

The `@copilotkit/react-ui` package provides ready-to-use React components for common AI interaction patterns, especially chat. These components save you a lot of time and effort:

1.  **Speed:** Add a polished AI chat interface to your app in minutes.
2.  **Consistency:** They offer a clean, standard look and feel (which you can customize!).
3.  **Integration:** They are designed to work seamlessly with the [CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md) and the core hooks we've learned about.
4.  **Common Patterns:** Components like `CopilotSidebar` (a chat window that docks to the side), `CopilotPopup` (a chat window that pops up), and `CopilotChat` (a flexible chat panel) cover most needs for direct AI interaction.

Let's see how easy it is to add an AI chat sidebar to your application.

**Meet the Main UI Components:**

*   `CopilotSidebar`: A chat interface that appears as a sidebar, often alongside your main app content.
*   `CopilotPopup`: A chat interface that pops up, usually triggered by a button.
*   `CopilotChat`: A more general chat panel component that can be embedded anywhere in your UI. It's the foundation for `CopilotSidebar` and `CopilotPopup`.

We'll focus on `CopilotSidebar` for our main example.

**How to Use `CopilotSidebar`**

Let's add a chat sidebar to a simple application.

**1. Installation:**
First, make sure you have the necessary packages installed. If you haven't already, add `@copilotkit/react-ui`:

```bash
npm install @copilotkit/react-core @copilotkit/react-ui
# or
yarn add @copilotkit/react-core @copilotkit/react-ui
```

**2. Import Styles:**
CopilotKit UI components come with default styling. To use them, import the CSS file in your main application file (e.g., `App.tsx` or `main.tsx`):

```tsx
// In your App.tsx or main.tsx
import "@copilotkit/react-ui/styles.css"; // <-- Add this line!

// ... rest of your imports and app setup
```
This ensures your chat components look good out of the box.

**3. Add the Component:**
Now, let's add `CopilotSidebar` to our app. Remember, it needs to be inside a `<CopilotKit>` provider, which we learned about in [Chapter 2: CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md).

```tsx
// App.tsx
import React from "react";
import { CopilotKit } from "@copilotkit/react-core";
import { CopilotSidebar } from "@copilotkit/react-ui";
import "@copilotkit/react-ui/styles.css"; // Already did this, but good reminder!

function App() {
  return (
    <CopilotKit runtimeUrl="/api/copilotkit"> {/* Provider setup */}
      <div id="app-content">
        <h1>My Awesome App</h1>
        <p>Welcome to my application!</p>
        {/* Your main application components would go here */}
      </div>

      <CopilotSidebar /> {/* <-- And here's our AI chat sidebar! */}
    </CopilotKit>
  );
}

export default App;
```

Breaking it down:
*   `import { CopilotSidebar } from "@copilotkit/react-ui";`: We import the sidebar component.
*   `<CopilotSidebar />`: We simply render the component within our `CopilotKit` provider.

**Input/Output:**
*   **What happens?** When you run your app, you'll see a chat icon or a docked sidebar. Clicking it opens a full chat interface.
*   **User Interaction:** Users can type messages into this sidebar.
*   **AI Interaction:** The sidebar handles sending messages to your AI backend (configured via `runtimeUrl` in `CopilotKit`) and displaying the AI's responses. It also works with any frontend or backend [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) you've defined.

That's it! You've added a fully functional AI chat sidebar.

**4. Simple Customization:**
You can easily customize some of the text in the chat interface using the `labels` prop.

```tsx
// App.tsx (showing only the CopilotSidebar part)
import { CopilotSidebar } from "@copilotkit/react-ui";

// ... inside your App component and CopilotKit provider
      <CopilotSidebar
        labels={{
          title: "My AI Assistant",
          initial: "Hello! Ask me anything about this app.",
        }}
      />
// ...
```
*   `labels.title`: Changes the title displayed at the top of the chat window.
*   `labels.initial`: Sets the initial greeting message from the AI.

These UI components offer many more customization options for appearance and behavior. You can check out the official documentation for more advanced styling and prop details.

**Other UI Components: `CopilotPopup` and `CopilotChat`**

*   **`CopilotPopup`**: Behaves similarly to `CopilotSidebar` but appears as a modal dialog or popup window, usually triggered by a floating action button.
    ```tsx
    // To use CopilotPopup, just replace CopilotSidebar:
    import { CopilotPopup } from "@copilotkit/react-ui";

    // ...
          <CopilotPopup labels={{ title: "Chat Helper" }} />
    // ...
    ```
    (See `CopilotKit/packages/react-ui/src/components/chat/Popup.tsx` for its simple structure.)

*   **`CopilotChat`**: This is a more fundamental component that provides the chat panel itself. `CopilotSidebar` and `CopilotPopup` use `CopilotChat` internally. You can use `CopilotChat` directly if you want to embed the chat interface within a specific part of your page layout, rather than as an overlay sidebar or popup.
    ```tsx
    // To use CopilotChat directly in a div:
    import { CopilotChat } from "@copilotkit/react-ui";

    // ...
          <div style={{ width: "300px", height: "500px", border: "1px solid #ccc" }}>
            <CopilotChat labels={{ title: "Embedded Chat" }}/>
          </div>
    // ...
    ```

**How Do They Work Under the Hood?**

These UI components are not magic! They are built using the same core concepts and hooks we've already discussed.

**Simplified Flow:**

1.  **Provider Power:** The UI component (e.g., `CopilotSidebar`) lives inside a `<CopilotKit>` provider, giving it access to the CopilotKit context (like your backend `runtimeUrl`).
2.  **Hook Magic:** Internally, components like `CopilotSidebar` and `CopilotChat` heavily rely on the `useCopilotChat` hook (from [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)). This hook manages the actual chat logic:
    *   Keeping track of messages.
    *   Handling user input.
    *   Sending messages to the AI backend.
    *   Receiving responses from the AI.
    *   Managing loading states.
3.  **Rendering UI:** The UI component then takes the state from `useCopilotChat` (like the list of messages) and renders the chat interface (message bubbles, input field, etc.).

Here's a sequence diagram showing how `CopilotSidebar` might work:

```mermaid
sequenceDiagram
    participant User
    participant CSidebar as CopilotSidebar UI
    participant UseCChat as useCopilotChat Hook
    participant CKitContext as CopilotKit Context
    participant BackendAI as CopilotRuntime Backend

    User->>CSidebar: Types "Hello AI!" and presses Send
    CSidebar->>UseCChat: Calls appendMessage("Hello AI!") (provided by the hook)
    UseCChat->>CKitContext: Accesses backend URL, action definitions
    UseCChat->>BackendAI: Sends message "Hello AI!" (+ context)
    BackendAI-->>UseCChat: AI processes and sends back "Hi there, User!"
    UseCChat->>UseCChat: Updates internal list of messages
    Note over UseCChat, CSidebar: Hook's state change triggers UI re-render
    CSidebar->>User: Displays "Hi there, User!" in the chat
```

**A Peek into the Code Structure:**

*   **`CopilotSidebar` and `CopilotPopup` are Wrappers:**
    If you look at the source code (e.g., `CopilotKit/packages/react-ui/src/components/chat/Sidebar.tsx` and `Popup.tsx`), you'll see they are relatively thin wrappers. They mainly manage their presentation (as a sidebar or popup) and delegate the core chat functionality to a component often called `CopilotModal` or directly to `CopilotChat`.

    ```tsx
    // Simplified from CopilotKit/packages/react-ui/src/components/chat/Popup.tsx
    // CopilotPopup passes its props to CopilotModal
    export function CopilotPopup(props: CopilotModalProps) {
      return <CopilotModal {...props} />; // Manages popup presentation
    }
    ```

    ```tsx
    // Simplified from CopilotKit/packages/react-ui/src/components/chat/Sidebar.tsx
    // CopilotSidebar also uses CopilotModal but adds sidebar-specific styling & behavior
    export function CopilotSidebar(props: CopilotModalProps) {
      // ... sidebar specific logic ...
      return (
        <div className="copilotKitSidebarContentWrapper">
          <CopilotModal {...props} /> {/* Manages sidebar presentation */}
        </div>
      );
    }
    ```
    Both `CopilotSidebar` and `CopilotPopup` ultimately rely on a shared modal or chat component that houses the actual chat logic.

*   **`CopilotChat` is the Core Engine:**
    The `CopilotChat` component (from `CopilotKit/packages/react-ui/src/components/chat/Chat.tsx`) is where the main chat UI and logic reside.

    ```tsx
    // Super simplified concept from CopilotKit/packages/react-ui/src/components/chat/Chat.tsx
    import { useCopilotChatLogic } from "./useCopilotChatLogic"; // Internal hook
    import { Messages } from "./Messages"; // Component to display messages
    import { Input } from "./Input"; // Component for text input

    export function CopilotChat(props: CopilotChatProps) {
      // This custom hook uses useCopilotChat from @copilotkit/react-core
      const { visibleMessages, isLoading, sendMessage } = useCopilotChatLogic(/*...*/);

      return (
        <div className="copilotKitChat"> {/* Main chat container */}
          <Messages messages={visibleMessages} inProgress={isLoading} /* ... */ />
          <Input onSend={sendMessage} inProgress={isLoading} /* ... */ />
          {/* Other elements like suggestions can also be here */}
        </div>
      );
    }
    ```
    Here:
    *   It uses a hook like `useCopilotChatLogic` (which itself wraps `useCopilotChat` from `@copilotkit/react-core`) to get the chat state (`visibleMessages`, `isLoading`) and functions (`sendMessage`).
    *   It then renders sub-components like `Messages` (to display the chat bubbles) and `Input` (for the user to type their messages).

So, these UI components cleverly package the power of the core hooks into easy-to-use, pre-styled elements!

**Benefits of Using `@copilotkit/react-ui` Components:**

*   **Rapid Development:** Get a functional AI chat interface up and running very quickly.
*   **Polished UI:** Comes with sensible default styles, making your AI copilot look professional.
*   **Customizable:** Offers props for common label changes and can be further styled with CSS.
*   **Best Practices:** Built on top of the core CopilotKit hooks and context, ensuring they work well within the ecosystem.

**Conclusion**

The UI components in `@copilotkit/react-ui`, like `CopilotSidebar`, `CopilotPopup`, and `CopilotChat`, are fantastic tools for quickly adding polished and functional AI chat interfaces to your React applications. They abstract away much of the boilerplate involved in building chat UIs from scratch, letting you focus on defining what your AI can *do*.

By leveraging these components, you save development time and ensure a consistent user experience for AI interactions.

So far, we've seen how to define what our AI can do ([Actions (Frontend & Backend)](01_actions__frontend___backend__.md)), how to power it up ([CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)), how to connect our UI to it programmatically ([Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)), and now, how to add pre-built chat UIs.

But what if you want AI assistance directly within a text input field, helping users write content? That's where our next topic comes in. Let's explore the [CopilotTextarea (`@copilotkit/react-textarea`)](05_copilottextarea____copilotkit_react_textarea___.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="05_copilottextarea____copilotkit_react_textarea___.md">
# Chapter 5: CopilotTextarea (`@copilotkit/react-textarea`)

In [Chapter 4: UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md), we saw how to quickly add pre-built chat interfaces to our applications. Those are great for conversational AI. But what if you want AI assistance *while* you're writing, directly within a text field? That's where the `CopilotTextarea` comes in!

**What's the Big Idea? A Superpowered Text Box**

Imagine you're writing an important email, and you're a bit stuck on how to phrase a sentence or what to write next. A standard `<textarea>` on a webpage is just a plain box for typing. It doesn't offer any help.

The `CopilotTextarea` from `@copilotkit/react-textarea` is different. It looks and feels like a regular textarea, but it's packed with AI smarts! It can suggest how to complete your sentences, help you rephrase text, or even generate content based on a prompt, all right where you're typing.

**What Problem Does It Solve?**

`CopilotTextarea` solves the problem of "writer's block" or needing a quick AI assist directly within input fields. It enhances the standard textarea experience by:

1.  **Providing AI-powered autosuggestions:** As you type, it can offer suggestions to complete your thoughts.
2.  **Offering content insertion and editing:** A handy hovering editor (usually opened with `Cmd+K` or `Ctrl+K`) lets you ask the AI to write new text, summarize, or edit selected text.
3.  **Leveraging application context:** It can use information from your application (made available via the `useCopilotReadable` hook from [Chapter 3: Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)) to make its suggestions even more relevant and intelligent.

Think of it as a drop-in replacement for a standard `<textarea>` that comes with its own mini AI writing assistant.

**How to Use `CopilotTextarea`**

Let's get started with a simple example. Imagine we're building a note-taking app for vacation planning.

**1. Installation (if you haven't already):**
You'll need the `@copilotkit/react-textarea` package, along with `@copilotkit/react-core` (which you should already have if you followed previous chapters).

```bash
npm install @copilotkit/react-core @copilotkit/react-textarea
# or
yarn add @copilotkit/react-core @copilotkit/react-textarea
```

**2. Import Styles:**
Like other CopilotKit UI components, `CopilotTextarea` comes with default styling. Import its CSS in your main application file (e.g., `App.tsx` or your root component):

```tsx
// In your App.tsx or main.tsx (or your root component)
import "@copilotkit/react-textarea/styles.css"; // <-- Add this line!
import "@copilotkit/react-ui/styles.css"; // If you use other UI components
// ... rest of your imports and app setup

// Remember to wrap your app in CopilotKitProvider
import { CopilotKit } from "@copilotkit/react-core";

function App() {
  return (
    <CopilotKit runtimeUrl="/api/copilotkit">
      {/* ... Your app content ... */}
    </CopilotKit>
  );
}
```

**3. Basic Usage:**
Using `CopilotTextarea` is very similar to using a regular `<textarea>`. You'll typically manage its value with React state.

Let's create a simple component for our vacation notes:

```tsx
// VacationNotes.tsx
import React, { useState } from "react";
import { CopilotTextarea } from "@copilotkit/react-textarea";

function VacationNotes() {
  const [notes, setNotes] = useState("");

  return (
    <div>
      <h2>My Vacation Ideas</h2>
      <CopilotTextarea
        value={notes}
        onValueChange={(newText) => setNotes(newText)}
        placeholder="What are your dream vacation plans?"
        autosuggestionsConfig={{
          textareaPurpose: "Brainstorming notes for an upcoming vacation. Focus on destinations and activities.",
        }}
      />
    </div>
  );
}

export default VacationNotes;
```

Let's break this down:
*   `import { CopilotTextarea } from "@copilotkit/react-textarea";`: We import the component.
*   `const [notes, setNotes] = useState("");`: Standard React state to hold the textarea's content.
*   `<CopilotTextarea ... />`: This is our AI-powered textarea.
    *   `value={notes}`: Binds the textarea's content to our `notes` state.
    *   `onValueChange={(newText) => setNotes(newText)}`: Updates our state when the user types or when AI makes changes. This prop is specific to `CopilotTextarea`.
    *   `placeholder="What are your dream vacation plans?"`: Just like a regular textarea placeholder.
    *   `autosuggestionsConfig={{ ... }}`: This is where we configure the AI magic!
        *   `textareaPurpose: "Brainstorming notes..."`: This is crucial! It tells the AI the general topic or goal of the text being written in this textarea. Clear purpose leads to better suggestions.

**Input/Output:**
*   **What happens?** You'll see a textarea. As you start typing, say "I want to go to...", the AI might suggest " a sunny beach destination" or " the mountains for hiking." The suggestion appears as faint text that you can accept (usually by pressing Tab).
*   If you select some text and press `Cmd+K` (Mac) or `Ctrl+K` (Windows), a small editing window pops up. You can type a command like "make this sound more exciting" or "list three activities for this place."

**Key Features in Action**

*   **Autosuggestions:**
    As you type, `CopilotTextarea` sends your current text (and the `textareaPurpose`) to the AI, which tries to predict what you might type next.
    *   For example, if `textareaPurpose` is "writing a professional email cover letter" and you type "I am writing to express my interest in the...", the AI might suggest "...Software Engineer position advertised on [Platform]."
    *   You can configure how quickly suggestions appear (`debounceTime`) and if they should be disabled when the textarea is empty (`disableWhenEmpty`) within the `autosuggestionsConfig`.

*   **Hovering Editor (Quick Edit / "Ask AI"):**
    This is a powerful feature. Select some text (or just place your cursor) and press the shortcut (default `Cmd+K` / `Ctrl+K`). A small input field appears.
    *   **Editing:** Select text, invoke the editor, and type "Rephrase this" or "Make this shorter."
    *   **Insertion/Generation:** Place your cursor, invoke the editor, and type "Write a paragraph about the best beaches in Italy" or "Suggest three packing essentials for a ski trip." The AI will generate and insert the text.

**Using Application Context**

Remember `useCopilotReadable` from [Chapter 3: Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)? If you've used it elsewhere in your app to make certain data available to the AI (e.g., a list of user preferences, previous travel history), `CopilotTextarea` can automatically leverage this context. This makes its suggestions and generations even more tailored and intelligent because it "knows" more about what the user is doing or what data is relevant in the application. You don't need to do anything extra in `CopilotTextarea` itself; if the context is available via `CopilotKitProvider`, it will be used.

**Configuration Deep Dive: `autosuggestionsConfig`**

The `autosuggestionsConfig` prop is your main control panel for the AI features. Here are some important sub-properties:

*   `textareaPurpose: string` (Required): As we've seen, this tells the AI the goal of this specific textarea. Be descriptive!
*   `contextCategories: string[]` (Optional): If you are using `useCopilotReadable` with categories, you can specify which categories of context are most relevant for this textarea. Defaults to all available context.
*   `chatApiConfigs`: This object configures the underlying calls to the AI model for different features.
    *   `suggestionsApiConfig`: Controls inline autosuggestions. You can set parameters like `maxTokens` (how long suggestions can be) or `stop` (characters that tell the AI to finish its suggestion, e.g., ".", "?", "!").
    *   `insertionApiConfig`: Configures AI calls when the hovering editor is used to *insert or generate new* text.
    *   `editingApiConfig`: Configures AI calls when the hovering editor is used to *edit existing* text.

Here's a slightly more detailed config example:

```tsx
// Inside your component, for the CopilotTextarea
autosuggestionsConfig={{
  textareaPurpose: "Drafting a polite follow-up email to a client after a meeting.",
  chatApiConfigs: {
    suggestionsApiConfig: { // For inline autocomplete
      maxTokens: 25,
      stop: [".", "\n"], // Stop suggestion at period or new line
    },
    insertionApiConfig: { // For "Ask AI" to generate text
      // You can specify model parameters here too if needed
    },
    editingApiConfig: { // For "Ask AI" to edit text
      // Model parameters for editing tasks
    }
  },
  debounceTime: 200, // Wait 200ms after user stops typing to get suggestions
  disableWhenEmpty: true, // No suggestions if textarea is empty
}}
```

You can find the full set of options in the `AutosuggestionsConfig` type definition within the CopilotKit library (see `CopilotKit/packages/react-textarea/src/types/autosuggestions-config/autosuggestions-config.tsx`). The example `CopilotKit/examples/next-openai/src/app/components/vacation-notes.tsx` also showcases several of these options.

**Under the Hood: How Does It Work?**

Let's peek behind the curtain.

**Simplified Flow for an Autosuggestion:**

1.  **User Types:** You type "My travel itinerary includes..." into the `CopilotTextarea`.
2.  **Debounce & Trigger:** After a short pause (the `debounceTime`), the component decides it's time to fetch a suggestion.
3.  **Context Gathering:** `CopilotTextarea` takes the current text, its `textareaPurpose`, and any relevant application context (provided globally by `CopilotKitProvider` and `useCopilotReadable`).
4.  **API Call:** It sends this information to your backend AI endpoint (specified in `CopilotKitProvider`'s `runtimeUrl`). This is handled by internal functions that prepare the request for the AI.
5.  **AI Processing:** Your backend, using the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md), forwards this to an LLM (like GPT). The LLM generates a plausible completion.
6.  **Suggestion Displayed:** The LLM's suggestion (e.g., "a visit to Paris and then Rome.") is sent back to the `CopilotTextarea`, which displays it as faint "ghost text."
7.  **User Accepts/Rejects:** You can press Tab to accept the suggestion or just keep typing to ignore it.

Here's a sequence diagram illustrating this:

```mermaid
sequenceDiagram
    participant User
    participant CTextarea as CopilotTextarea
    participant CKitContext as CopilotKit Context
    participant BackendRuntime as CopilotRuntime (Backend)
    participant LLM

    User->>CTextarea: Types "My travel itinerary includes..."
    Note over CTextarea: Debounce timer starts
    Note over CTextarea: Timer finishes
    CTextarea->>CKitContext: Gets textareaPurpose, app context
    CTextarea->>BackendRuntime: Sends current text + all context for suggestion
    BackendRuntime->>LLM: Asks for text completion
    LLM-->>BackendRuntime: Suggests " a visit to Paris and then Rome."
    BackendRuntime-->>CTextarea: Returns suggestion
    CTextarea->>User: Displays suggestion as ghost text
```

The hovering editor works similarly: it takes the selected text (if any), your instruction (e.g., "summarize this"), and context, then sends it to the AI for processing.

**A Glimpse into the Code Structure:**

*   The `CopilotTextarea` component you import (from `CopilotKit/packages/react-textarea/src/components/copilot-textarea/copilot-textarea.tsx`) is a smart wrapper.
*   It internally uses a more fundamental component called `BaseCopilotTextarea`.
*   Before rendering `BaseCopilotTextarea`, `CopilotTextarea` sets up the AI interaction functions using hooks like:
    *   `useMakeStandardAutosuggestionFunction`: This hook takes your `textareaPurpose`, `contextCategories`, and `suggestionsApiConfig` to create the actual function that will be called to get autosuggestions.
    *   `useMakeStandardInsertionOrEditingFunction`: Similarly, this prepares the function for handling insertions and edits from the hovering editor, using the `insertionApiConfig` and `editingApiConfig`.
*   The `autosuggestionsConfig` you provide is merged with default configurations (`merge(defaultAutosuggestionsConfig, autosuggestionsConfigUserSpecified)`) to ensure all necessary settings are present.
*   `BaseCopilotTextarea` then handles the user input events, manages the display of suggestions, integrates the hovering editor UI, and calls the prepared AI functions when needed.

These prepared functions ultimately use the core communication mechanisms provided by `CopilotKitProvider` (like the `runtimeUrl` to know where your AI backend is) to interact with the LLM.

**Conclusion**

The `CopilotTextarea` is a powerful component that seamlessly blends AI assistance into the familiar textarea element. It empowers users by providing intelligent autosuggestions, easy content generation, and quick editing capabilities, all within their typing flow.

Key takeaways:
*   It's a drop-in replacement for `<textarea>` with added AI superpowers.
*   Configure its behavior primarily through the `autosuggestionsConfig` prop, especially the `textareaPurpose`.
*   It offers both inline autosuggestions and a hovering editor for more complex AI interactions.
*   It automatically benefits from application-wide context provided by `useCopilotReadable`.

Now that you've seen how AI can enhance UI components on the frontend, you might be wondering about the backend engine that powers all these AI interactions. Let's dive into that next, with [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="06_copilotruntime__backend_engine__.md">
# Chapter 6: CopilotRuntime (Backend Engine)

In [Chapter 5: CopilotTextarea (`@copilotkit/react-textarea`)](05_copilottextarea____copilotkit_react_textarea___.md), we saw how AI can supercharge a simple textarea, offering suggestions and helping with writing. You might be wondering: where does all that intelligence actually come from? How does the `CopilotTextarea`, or the chat UIs from [Chapter 4: UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md), actually talk to an AI and get smart responses?

The answer lies in the **CopilotRuntime**, the backend "brain" of your CopilotKit application!

**What's the Big Idea? The AI's Command Center**

Imagine your AI copilot is like a sophisticated robot. The frontend components we've discussed (like chat windows or the smart textarea) are its eyes, ears, and voice. But the robot needs a central processing unit (CPU) to understand requests, think, and decide what to do.

The **CopilotRuntime** is that CPU. It's a piece of software that lives on your server (your backend, which could be built with Node.js, Python, Next.js, etc.). It's the engine that drives the AI logic.

**What Problem Does It Solve?**

The CopilotRuntime acts as the central coordinator for all AI-related tasks:

1.  **Conversation Management:** It keeps track of the conversation between the user and the AI.
2.  **LLM Communication:** It's responsible for actually talking to Large Language Models (LLMs) like OpenAI's GPT, Anthropic's Claude, or others.
3.  **Action Orchestration:** Remember [Actions (Frontend & Backend)](01_actions__frontend___backend__.md)? The CopilotRuntime receives information about *all* available actions (both frontend and backend). It tells the LLM about these tools. When the LLM decides to use an action, the CopilotRuntime either executes backend actions directly or tells the frontend to execute a frontend action.
4.  **Agent Integration:** It can work with more advanced [Agents (Backend Focus)](07_agents__backend_focus__.md) to handle complex, multi-step tasks (we'll learn more about this in the next chapter!).

Think of it as the **air traffic controller** for AI requests. It ensures that user input is correctly understood, the right AI model is consulted, appropriate actions are taken, and a coherent response is sent back to the user. Without it, your frontend AI components wouldn't have anyone to talk to!

**Setting Up Your CopilotRuntime**

Let's see how you'd typically set up a CopilotRuntime on your backend. The exact code depends on your backend language and framework (e.g., Node.js with Express, Next.js API Routes, Python with FastAPI).

The core idea is to:
1.  Instantiate the runtime.
2.  Define any backend actions it should know about.
3.  Connect it to an LLM using a [Service Adapters (Backend)](08_service_adapters__backend__.md).
4.  Expose it as an HTTP endpoint. This endpoint URL is what you provide to the `CopilotKitProvider` in your frontend (as the `runtimeUrl` prop from [Chapter 2: CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)).

### Example: Node.js (using `@copilotkit/runtime`)

Here's a highly simplified example for a Node.js backend. Let's imagine we want a backend action that can look up a product price.

```typescript
// In your backend server file (e.g., server.ts or index.ts)
import { CopilotRuntime, OpenAIAdapter } from "@copilotkit/runtime";
// import { createServer } from "node:http"; // For a basic Node HTTP server
// import { copilotRuntimeNodeHttpEndpoint } from "@copilotkit/runtime"; // Helper

// 1. Instantiate the CopilotRuntime
const runtime = new CopilotRuntime({
  // 2. Define backend actions
  actions: [
    {
      name: "getProductPrice",
      description: "Gets the price of a product.",
      parameters: [{ name: "productName", type: "string", required: true }],
      handler: async ({ productName }) => {
        // In a real app, you'd look this up in a database
        if (productName.toLowerCase() === "super widget") return "$99.99";
        return "Sorry, product not found.";
      },
    },
  ],
});

// 3. Connect to an LLM (e.g., OpenAI)
// Ensure your OPENAI_API_KEY environment variable is set
// const serviceAdapter = new OpenAIAdapter(); // Uses OpenAI

// 4. Expose as an HTTP endpoint (simplified concept)
// const handler = copilotRuntimeNodeHttpEndpoint({
//   runtime,
//   serviceAdapter,
//   endpoint: "/api/copilotkit", // This becomes your runtimeUrl
// });
// createServer(handler).listen(4000);
// console.log("Backend AI runtime listening on http://localhost:4000/api/copilotkit");
```
Let's break this down:
*   `new CopilotRuntime(...)`: We create an instance of our backend engine.
*   `actions: [...]`: We pass an array of backend action definitions. Each action has a `name`, `description`, `parameters` it expects, and a `handler` function that contains the actual logic to execute on the server.
*   `OpenAIAdapter()`: This part (which we'll cover in detail in [Service Adapters (Backend)](08_service_adapters__backend__.md)) tells the runtime *how* to talk to a specific LLM, in this case, OpenAI's models.
*   `copilotRuntimeNodeHttpEndpoint`: This helper (or similar ones for Next.js, etc.) takes your runtime and service adapter and creates an HTTP request handler. This handler is what listens for requests from your frontend. The `endpoint` path (`/api/copilotkit`) is what you'd use as `runtimeUrl` in your frontend's `CopilotKitProvider`.

**Input/Output:**
*   **Frontend asks (via `runtimeUrl`):** "What's the price of a Super Widget?"
*   **CopilotRuntime (with LLM's help):** Understands the intent, sees the `getProductPrice` action, and figures out `productName` is "Super Widget".
*   **Action Execution:** Runs the `handler` for `getProductPrice`.
*   **Result:** The handler returns "$99.99".
*   **LLM generates response:** "The price of a Super Widget is $99.99."
*   **CopilotRuntime streams back to frontend:** The frontend UI displays the answer.

You can see a more complete Node.js example in `CopilotKit/examples/node-http/src/index.ts` or for Next.js in `CopilotKit/examples/next-openai/src/app/api/copilotkit/route.ts`.

### Example: Python (using the `copilotkit` SDK)

The concept is similar in Python. You'd typically use the `CopilotKitRemoteEndpoint` class.

```python
# In your Python backend (e.g., main.py)
from copilotkit import CopilotKitRemoteEndpoint, Action
# from copilotkit.integrations.fastapi import add_fastapi_endpoint # For FastAPI
# from fastapi import FastAPI # For FastAPI

# Dummy handler function for our action
async def get_product_price_handler(productName: str):
    if productName.lower() == "pythonic gadget":
        return "€42.00"
    return "Sorry, product not found in Python land."

# 1. Instantiate CopilotKitRemoteEndpoint (similar to CopilotRuntime)
copilot_sdk = CopilotKitRemoteEndpoint(
    # 2. Define backend actions
    actions=[
        Action(
            name="getProductPricePython",
            description="Gets the price of a product from the Python backend.",
            parameters=[
                {"name": "productName", "type": "string", "description": "Product name"}
            ],
            handler=get_product_price_handler
        )
    ]
)

# 3. Connect to an LLM: This is handled differently.
# The Python SDK often works with a frontend that already has a service adapter.
# The primary role here is to expose actions and agents.

# 4. Expose as an HTTP endpoint (e.g., with FastAPI)
# app = FastAPI()
# add_fastapi_endpoint(
#   app,
#   copilot_sdk,
#   "/api/copilotkit_python" # This becomes your runtimeUrl
# )
# # Then run with Uvicorn: uvicorn main:app --reload
# print("Python backend AI runtime likely on http://localhost:8000/api/copilotkit_python")
```
Breakdown:
*   `CopilotKitRemoteEndpoint(...)`: This is the Python equivalent for setting up your backend logic.
*   `Action(...)`: Defines a backend action, similar to the TypeScript version.
*   `add_fastapi_endpoint`: A helper to integrate with the FastAPI web framework, making your actions available at a specific URL.

You can find more details in the Python SDK documentation, like `sdk-python/copilotkit/sdk.py`.

**How It Works: The Journey of a Request**

Let's trace what happens when a user interacts with a CopilotKit-powered frontend component (like `CopilotSidebar` or `CopilotTextarea`):

1.  **User Input:** The user types a message (e.g., "Add 'buy milk' to my to-do list and save it").
2.  **Frontend to Backend:** The frontend (via `CopilotKitProvider`) packages this message, along with descriptions of any *frontend* [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) (like `addTaskToUI` from Chapter 1), and sends it all to the `runtimeUrl` (your CopilotRuntime backend endpoint).
3.  **CopilotRuntime Receives:** Your backend (Node.js, Python, etc.) receives this HTTP request.
4.  **Context Preparation:** The CopilotRuntime combines:
    *   The user's message.
    *   Descriptions of frontend actions (sent from the client).
    *   Descriptions of its own backend actions (like `getProductPrice` or `saveTaskPersistent`).
5.  **LLM Interaction:** The CopilotRuntime, using a [Service Adapters (Backend)](08_service_adapters__backend__.md) (e.g., `OpenAIAdapter`), sends all this context to an LLM (like GPT). It essentially asks the LLM, "Based on this conversation and these available tools (actions), what should we do?"
6.  **LLM Decides:** The LLM analyzes the request and might decide:
    *   To just generate a text response.
    *   To use one or more of the available actions. If so, it specifies which action(s) and what arguments to use (e.g., "call `saveTaskPersistent` with `task_text` = 'buy milk'").
7.  **Action Execution:**
    *   **Backend Action:** If the LLM chose a backend action, the CopilotRuntime executes its handler function directly on the server.
    *   **Frontend Action:** If it's a frontend action, the CopilotRuntime sends a specific instruction back to the client, telling it which frontend action to run and with what arguments. The client (using hooks like `useCopilotAction`) then executes it.
8.  **Result to LLM (if action executed):** The result of the action (e.g., "Task saved successfully" or data from a database) is sent back to the LLM. This helps the LLM formulate a more informed final response.
9.  **Final Response Generation:** The LLM generates a final, natural language response for the user.
10. **Stream to Frontend:** The CopilotRuntime streams this response back to the frontend, which then displays it in the UI.

Here's a simplified diagram:

```mermaid
sequenceDiagram
    participant User
    participant FrontendUI as Frontend UI (React)
    participant CRuntime as CopilotRuntime (Backend)
    participant SAdapter as Service Adapter (Backend)
    participant LLM (Backend)
    participant BackendActionFn as Backend Action Handler

    User->>FrontendUI: "Save 'Buy Groceries' to DB"
    FrontendUI->>CRuntime: Sends message & list of frontend/backend actions
    CRuntime->>SAdapter: Prepares request for LLM
    SAdapter->>LLM: User message + all actions info
    LLM-->>SAdapter: Decides: Call 'saveTaskPersistent' with 'Buy Groceries'
    SAdapter-->>CRuntime: LLM's decision
    CRuntime->>BackendActionFn: Executes saveTaskPersistent('Buy Groceries')
    BackendActionFn-->>CRuntime: Returns "Task saved."
    CRuntime->>SAdapter: Sends action result to LLM
    SAdapter->>LLM: "Action result: Task saved."
    LLM-->>SAdapter: Generates final text: "Okay, I've saved 'Buy Groceries'."
    SAdapter-->>CRuntime: Final text
    CRuntime-->>FrontendUI: Streams response
    FrontendUI->>User: Displays "Okay, I've saved 'Buy Groceries'."
```

**A Peek into the Code (Conceptual)**

*   **`CopilotRuntime` Class (TypeScript):**
    (From `CopilotKit/packages/runtime/src/lib/runtime/copilot-runtime.ts`)
    When you instantiate `new CopilotRuntime({ actions: [...] })`, you're providing the list of backend functions the AI can call.
    ```typescript
    // Simplified constructor concept
    export class CopilotRuntime {
      public actions: Action<any>[]; // Stores your backend actions
      // ... other properties for remote agents, langserve, middleware ...

      constructor(params?: CopilotRuntimeConstructorParams) {
        this.actions = params?.actions || [];
        // ... initialization of other features ...
      }

      // Core method that handles incoming requests
      async processRuntimeRequest(request: CopilotRuntimeRequest): Promise<CopilotRuntimeResponse> {
        // 1. Gets server-side actions (including any from remote_endpoints or mcpServers)
        // 2. Prepares messages and actions for the LLM
        // 3. Calls serviceAdapter.process(...) to talk to LLM
        // 4. Handles LLM's decision (text response or action call)
        // 5. Returns a streamable response (eventSource)
        // ... lots of logic here ...
      }
    }
    ```
    The `processRuntimeRequest` method is the heart of the runtime, orchestrating the flow described above. It works closely with a [Service Adapters (Backend)](08_service_adapters__backend__.md) to communicate with the chosen LLM.

*   **`CopilotKitRemoteEndpoint` Class (Python):**
    (From `sdk-python/copilotkit/sdk.py`)
    Similarly, this class in the Python SDK holds your backend actions and agent definitions.
    ```python
    # Simplified constructor concept
    class CopilotKitRemoteEndpoint:
        def __init__(
            self,
            *,
            actions: Optional[List[Action]] = None,
            agents: Optional[List[Agent]] = None,
        ):
            self.actions = actions or [] # Stores your backend actions
            self.agents = agents or []   # Stores your backend agents

        # Method to execute a specific action
        async def execute_action(self, name: str, arguments: dict, context: CopilotKitContext):
            action_to_execute = self._get_action(name=name, context=context)
            # ... calls action_to_execute.execute(arguments) ...
            # ... (actual method returns a Coroutine)
            pass
        
        # There are also methods for execute_agent, info, etc.
    ```
    When an HTTP request comes to an endpoint managed by `CopilotKitRemoteEndpoint` (e.g., via a FastAPI integration), the SDK routes it to methods like `execute_action` if the LLM decided to call one of your Python actions.

**Conclusion**

The CopilotRuntime is the indispensable backend engine that powers your AI copilot's intelligence. It lives on your server, manages conversations, talks to LLMs, and intelligently decides when and how to use the tools (Actions) you've provided—whether those actions live on the frontend or the backend.

Key Takeaways:
*   It's the server-side "brain" of CopilotKit.
*   You configure it with your backend actions and connect it to an LLM via a Service Adapter.
*   It's exposed as an HTTP endpoint that your frontend communicates with (the `runtimeUrl`).
*   It orchestrates the entire AI interaction flow, from understanding user input to generating responses and executing tasks.

While simple actions are powerful, sometimes you need more complex, stateful, and long-running AI logic. For that, CopilotKit offers the concept of Agents. Let's explore them in the next chapter: [Agents (Backend Focus)](07_agents__backend_focus__.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="07_agents__backend_focus__.md">
# Chapter 7: Agents (Backend Focus)

In [Chapter 6: CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md), we learned about the backend engine that powers our AI interactions and executes actions. That's great for single, well-defined tasks. But what if you need your AI to perform more complex, multi-step operations, almost like a dedicated assistant for a specific job? That's where **Agents** come into play!

**What's the Big Idea? Your AI Specialist**

Imagine you want to build an AI research assistant. You might ask it: "Hey Copilot, find the latest trends in solar panel technology, compare their efficiency, and write a short summary for me."

This isn't a single action. It involves:
1.  Searching for "latest trends in solar panel technology."
2.  Perhaps searching for "solar panel efficiency comparisons."
3.  Reading and understanding multiple sources.
4.  Synthesizing this information.
5.  Finally, writing a summary.

A simple action can't do all that. You need something smarter, something that can follow a plan, use different tools, and maybe even remember what it has learned along the way. That's an **Agent**.

**What Problem Do Agents Solve?**

Agents are more advanced AI entities designed for:

1.  **Complex, Multi-Step Tasks:** Like our research assistant example.
2.  **State Management/Memory:** Agents can often keep track of information over time or across several steps (e.g., remembering previous search results).
3.  **Tool Usage (Actions):** They can use the [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) we defined earlier as tools to achieve parts of their goal.
4.  **Orchestration:** Some agents can even coordinate other agents or specialized services.

Think of them as specialized AI workers you can hire for particular jobs. CopilotKit allows you to integrate these advanced agents, especially those built with popular frameworks like LangGraph (for building stateful, multi-actor applications with LLMs) or CrewAI (for orchestrating role-playing, autonomous AI agents). These agents are primarily managed and executed on your **backend** using CopilotKit's Python or JavaScript SDKs.

**Agents in CopilotKit: The Backend Powerhouse**

While your frontend (using components like `CopilotSidebar` from [UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md)) is where the user interacts, the heavy lifting for agents happens on the server.

Let's look at how you might define and integrate an agent, conceptually, using Python (as LangGraph and CrewAI are popular Python libraries).

### 1. The Basic Idea: An `Agent` Contract

CopilotKit's Python SDK provides a base `Agent` class. If you were building an agent from scratch (or an adapter for a new framework), it would need to implement a couple of key methods.

```python
# Conceptual: from sdk-python/copilotkit/agent.py
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
# from .types import Message # For full type hints

class Agent(ABC):
    def __init__(self, name: str, description: Optional[str] = None):
        self.name = name
        self.description = description
        # Validation for agent name...

    @abstractmethod
    def execute(self, state: Dict, messages: List[Message], **kwargs): # Simplified
        # This is where the agent does its work
        pass

    @abstractmethod
    async def get_state(self, thread_id: str):
        # This is how an agent reports its current status/memory
        pass
```
*   `name`: A unique identifier for the agent.
*   `description`: Helps CopilotKit (and potentially an LLM) understand what this agent does and when to use it.
*   `execute(...)`: The main function that gets called to run the agent. It receives the current conversation `messages`, the agent's current `state`, and other parameters. This function would contain the logic for the agent to plan, use tools, and achieve its goal.
*   `get_state(...)`: A function to retrieve the current state of the agent for a given conversation or task `thread_id`.

This defines the basic "contract" an agent needs to fulfill to work with CopilotKit.

### 2. Integrating with LangGraph or CrewAI

More commonly, you'll use established agent frameworks. CopilotKit provides convenient wrappers like `LangGraphAgent` and `CrewAIAgent` to integrate agents built with these tools.

**Example: Using `LangGraphAgent` (Conceptual)**

LangGraph allows you to define agents as "graphs" where nodes represent steps or actors, and edges represent transitions.

```python
# Conceptual: based on sdk-python/copilotkit/langgraph_agent.py
from copilotkit import LangGraphAgent
# Assume 'my_research_graph' is a compiled LangGraph graph
# from langgraph.graph.graph import CompiledGraph
# my_research_graph: CompiledGraph = ... # Your LangGraph definition

research_agent = LangGraphAgent(
    name="researcher",
    description="An agent that researches topics and provides summaries.",
    graph=my_research_graph, # Your actual LangGraph agent
    # langgraph_config can also be provided here
)
```
*   `LangGraphAgent(...)`: We wrap our existing LangGraph agent (`my_research_graph`).
*   `name` and `description`: Tell CopilotKit about this agent.
*   `graph`: This is your pre-built LangGraph. The complexity of building this graph (defining its nodes, edges, logic using LLMs and tools) is part of using LangGraph itself. CopilotKit just needs the final, compiled graph to integrate it.

The `CrewAIAgent` (from `sdk-python/copilotkit/crewai/crewai_agent.py`) works similarly, allowing you to plug in a CrewAI `Crew` or `Flow`.

### 3. Exposing Agents via CopilotRuntime

Once you have your agent defined (e.g., `research_agent`), you tell your [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) about it.

In Python, using `CopilotKitRemoteEndpoint` (which is part of the backend setup):

```python
# In your Python backend (e.g., main.py where you set up FastAPI)
from copilotkit import CopilotKitRemoteEndpoint
# from .my_agents import research_agent # Assuming research_agent is defined elsewhere

copilot_sdk = CopilotKitRemoteEndpoint(
    agents=[research_agent] # Add your agent here
    # You can also list backend actions here
)

# ... then expose copilot_sdk via FastAPI or another framework ...
```
By including `research_agent` in the `agents` list, you make it available for the CopilotKit system to use. Your frontend can now potentially trigger this agent.

### 4. Frontend Interaction: `useCoAgent`

On the frontend, you'd typically use the `useCoAgent` hook from [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md) to interact with a backend agent. This hook allows your React components to:

*   Connect to a specific backend agent by its `name`.
*   Send initial instructions or data to the agent.
*   Display the agent's current `state`.
*   Allow the user to update parts of the state, which then synchronizes back to the backend agent.
*   Trigger the agent to `run` or `continue` its process.

```tsx
// Simplified frontend component (e.g., ResearchComponent.tsx)
import { useCoAgent } from "@copilotkit/react-core";

interface ResearchState {
  topic?: string;
  summary?: string;
  status?: string;
}

function ResearchManager() {
  const { state, setState, run } = useCoAgent<ResearchState>({
    name: "researcher", // Matches backend agent name
    initialState: { topic: "", summary: "Not started", status: "idle" },
  });

  const handleResearch = () => {
    // 'run' will trigger the backend "researcher" agent
    // The agent will use 'state.topic' and update 'state.summary' etc.
    if (state.topic) run();
  };

  return (
    <div>
      <input
        value={state.topic}
        onChange={(e) => setState(prev => ({ ...prev, topic: e.target.value }))}
        placeholder="Enter research topic"
      />
      <button onClick={handleResearch}>Start Research</button>
      <p>Status: {state.status}</p>
      <textarea readOnly value={state.summary} />
    </div>
  );
}
```
*   `useCoAgent({ name: "researcher", ... })`: Connects to our backend agent.
*   `state`: Holds the shared state between the frontend and the backend agent (e.g., the research topic, the current summary, status).
*   `setState`: Allows the UI to update the state (e.g., when the user types a topic). This change is also sent to the backend agent.
*   `run()`: This function signals the backend agent to start or continue its execution. The agent will pick up the current `state` (including the `topic`) and begin its work. As the backend agent works (e.g., performing searches, writing summaries), it will update its state, and these updates will be reflected back in the `state` object on the frontend, causing the UI to re-render.

**Input/Output:**
*   **User types:** "Solar panel efficiency" into the input field and clicks "Start Research."
*   **Frontend (`useCoAgent`):** Calls `run()`. This sends a request to the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md), indicating the "researcher" agent should run with the current state (which includes `topic: "Solar panel efficiency"`).
*   **Backend Agent (`research_agent`):** Its `execute` method (or LangGraph/CrewAI flow) kicks off. It uses tools (Actions) to search, analyze, etc. As it makes progress, it updates its internal state (e.g., `status: "Searching..."`, then `status: "Summarizing..."`, `summary: "..."`).
*   **State Sync:** These state changes are streamed back to the frontend via CopilotKit's communication channel.
*   **Frontend UI:** The `ResearchManager` component re-renders, showing the updated status and summary.

**How It Works Under the Hood: Agent Execution Flow**

Let's simplify the journey of an agent interaction:

1.  **Frontend Initiation:** The user interacts with a UI element (e.g., clicks a button) that calls a function from `useCoAgent` (like `run` or `setState` which implicitly might trigger a run).
2.  **Request to Backend:** The frontend, via the `CopilotKitProvider`'s `runtimeUrl`, sends a request to your [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md). This request specifies:
    *   The `agentName` to execute (e.g., "researcher").
    *   The current `threadId` (a unique ID for this ongoing task or conversation).
    *   The latest `state` from the frontend (e.g., `{ topic: "Solar panel efficiency" }`).
    *   Any new `messages` from the user.
    *   The `AgentSessionInput` (from `CopilotKit/packages/runtime/src/graphql/inputs/agent-session.input.ts`) is an example of how the frontend might structure part of this request.
3.  **Backend Runtime Routes to Agent:** The CopilotRuntime identifies the correct agent (e.g., our `research_agent` which is a `LangGraphAgent`).
4.  **Agent Execution:**
    *   The `execute` method of the agent wrapper (e.g., `LangGraphAgent.execute`) is called.
    *   This method takes the incoming `state`, `messages`, etc., and typically:
        *   Merges the new frontend state with the agent's existing internal state for that `threadId`.
        *   Passes control to the underlying agent framework (LangGraph or CrewAI).
        *   The LangGraph/CrewAI agent then runs its defined logic: calling LLMs, using tools (which could be CopilotKit [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) you've defined elsewhere), making decisions.
5.  **State Updates & Streaming:** As the agent operates, it updates its state. The `LangGraphAgent` (for example) is designed to stream these state changes and other events (like intermediate tool outputs or LLM thoughts, if configured) back to the CopilotRuntime.
6.  **Runtime Relays to Frontend:** The CopilotRuntime forwards these streamed updates to the connected frontend client.
7.  **Frontend Reacts:** The `useCoAgent` hook on the frontend receives these state updates, updates its local `state` object, causing the React component to re-render and display the latest information (e.g., progress status, partial results, final summary).

Here’s a diagram of this flow:

```mermaid
sequenceDiagram
    participant User
    participant FrontendUI as Frontend UI (React with useCoAgent)
    participant CRuntime as CopilotRuntime (Backend)
    participant CKAgent as CopilotKit Agent (e.g., LangGraphAgent)
    participant ActualAgent as LangGraph/CrewAI Logic

    User->>FrontendUI: Enters topic, clicks "Start Research"
    FrontendUI->>CRuntime: runAgent("researcher", currentState, threadId)
    CRuntime->>CKAgent: Calls execute(currentState, messages, threadId)
    CKAgent->>ActualAgent: Passes control (state, inputs)
    ActualAgent->>ActualAgent: Executes steps (uses LLMs, Tools/Actions)
    ActualAgent-->>CKAgent: Intermediate state updates / results
    CKAgent-->>CRuntime: Streams state updates
    CRuntime-->>FrontendUI: Streams state updates
    FrontendUI->>FrontendUI: Re-renders with new state (e.g., shows summary)
    FrontendUI->>User: Displays updated information
```

**Diving Deeper into Code (Conceptual):**

*   **`LangGraphAgent.execute` (from `sdk-python/copilotkit/langgraph_agent.py`):**
    The `execute` method in `LangGraphAgent` is responsible for bridging CopilotKit's request to the LangGraph stream.
    ```python
    # Simplified from sdk-python/copilotkit/langgraph_agent.py
    # class LangGraphAgent(Agent):
    async def _stream_events( # Called by execute
            self,
            state: Any, # Current state from frontend/CopilotKit
            messages: List[Message], # User messages
            thread_id: str,
            # ... other params ...
        ):
        # 1. Get current LangGraph state for thread_id
        agent_state = await self.graph.aget_state(config_with_thread_id)
        
        # 2. Merge CopilotKit state and messages into LangGraph state
        #    (using self.merge_state and self.convert_messages)
        merged_langgraph_state = self.merge_state(
            state=agent_state.values, messages=self.convert_messages(messages), ...
        )

        # 3. Stream events from the LangGraph graph
        async for event in self.graph.astream_events(merged_langgraph_state, ...):
            # Process 'event' (could be LLM token, tool call, state update)
            # If it's a state update relevant to CopilotKit (on_copilotkit_state_sync),
            # yield it back to CopilotRuntime.
            if event_is_state_sync:
                yield self._emit_state_sync_event(...) # Formats for CopilotKit
            
            # Yield other LangGraph events as well
            yield langchain_dumps(event) + "\n"
        # ... final state emission ...
    ```
    This shows how `LangGraphAgent` prepares the input for the LangGraph `graph` and then iterates over the events produced by the graph, forwarding relevant information back.

*   **`useCoAgent` Frontend Logic (from `CopilotKit/packages/react-core/src/hooks/use-coagent.ts`):**
    The `runAgent` function (called internally when you invoke `run()` from `useCoAgent`) is what kicks off the process from the frontend.
    ```typescript
    // Simplified from useCoAgent hook
    async function runAgent(
      name: string, // Agent name, e.g., "researcher"
      context: CopilotContextParams & CopilotMessagesContextParams,
      appendMessage: (message: Message) => Promise<void>,
      runChatCompletion: () => Promise<Message[]>, // This is key
      hint?: HintFunction,
    ) {
      // ... set agent session ...
      
      // If a hint is provided (e.g., why we're re-running), append it as a message
      if (hint) {
        const hintMessage = hint({ /* previousState, currentState */ });
        if (hintMessage) await appendMessage(hintMessage);
      }
      
      // Crucially, it triggers the standard chat completion flow.
      // The CopilotRuntime backend, when processing this chat completion,
      // will see that an agent session is active (e.g., for "researcher").
      // It will then route the request to the agent's 'execute' method
      // instead of just a simple LLM call.
      await runChatCompletion();
    }
    ```
    When `runChatCompletion()` is called while an agent session is active (set by `useCoAgent` via `setAgentSession`), the backend [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) knows to invoke the specified agent (`name`) with the current messages and state. The agent's execution (and any state updates it generates) are then streamed back.

**Conclusion**

Agents are your solution for building sophisticated AI capabilities that go beyond simple question-answering or single-shot actions. They allow for complex, multi-step task execution, often maintaining memory and using a variety of tools. CopilotKit focuses on making it easy to integrate agents built with powerful backend frameworks like LangGraph and CrewAI into your application, and to connect them with your frontend using hooks like `useCoAgent` for a dynamic, stateful user experience.

Key Takeaways:
*   Agents handle complex, multi-step tasks with state.
*   They are primarily defined and run on the backend (Python/JS).
*   CopilotKit helps integrate agents from frameworks like LangGraph or CrewAI.
*   The `useCoAgent` hook on the frontend allows interactive, stateful communication with backend agents.
*   The [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) orchestrates the execution of these agents.

Now that we've seen how to build actions and complex agents, you might wonder how the CopilotRuntime actually talks to different Large Language Models (like OpenAI, Anthropic, etc.). That's what we'll explore next in [Chapter 8: Service Adapters (Backend)](08_service_adapters__backend__.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="08_service_adapters__backend__.md">
# Chapter 8: Service Adapters (Backend)

In the [previous chapter on Agents (Backend Focus)](07_agents__backend_focus__.md), we saw how to create powerful, multi-step AI assistants. Both simple [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) and complex Agents ultimately need to communicate with a Large Language Model (LLM) to get their intelligence. But there are many different LLMs out there – OpenAI's GPT models, Anthropic's Claude, Google's Gemini, models on Groq, and more!

How does your CopilotKit application talk to these different "brains" without you having to rewrite your backend code every time you want to try a new LLM? That's where **Service Adapters** come in!

**What's the Big Idea? The Universal Remote for LLMs**

Imagine you have a universal TV remote. You want it to control your Sony TV, your Samsung soundbar, and your LG Blu-ray player. Each device "speaks" a different command language. Your universal remote needs special codes or settings for each brand to translate your button presses (like "Volume Up") into commands each specific device understands.

**Service Adapters** in CopilotKit are just like that!
*   Your **CopilotKit application** (specifically the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md)) is like the **universal remote**.
*   Different **LLMs** (OpenAI, Anthropic, Google, Groq) are like the different **TV brands**.
*   A **Service Adapter** for a specific LLM (e.g., `OpenAIAdapter`, `AnthropicAdapter`) is like the **special code** that allows your CopilotKit "remote" to "talk" to that particular LLM "brand."

Each LLM provider has its own unique way of receiving requests (e.g., the format of messages, how to specify tools/actions) and sending back responses. Service Adapters handle this translation, allowing your core CopilotKit logic to remain the same, regardless of which LLM you're using.

**Why Do We Need Them? Speaking Different AI Languages**

When your [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) needs an LLM to process a user's message or decide on an action, it prepares a standard request. However:
*   OpenAI's API expects this request in one format.
*   Anthropic's API expects it in another.
*   Google's Gemini API in yet another.

Without Service Adapters, you'd have to write custom code for each LLM you want to support. This would be complex and hard to maintain. Service Adapters solve this by providing a plug-and-play way to connect to various LLMs.

**How CopilotKit Uses Service Adapters**

On your backend, you'll have an instance of the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md). This runtime is responsible for managing conversations and orchestrating actions. When it needs to interact with an LLM, it uses a Service Adapter that you provide.

You typically:
1.  Instantiate `CopilotRuntime` (perhaps with your backend actions or agents).
2.  Instantiate your chosen `ServiceAdapter` (e.g., `OpenAIAdapter`).
3.  Pass *both* of these to your HTTP endpoint handler logic. This handler is the URL your frontend communicates with (the `runtimeUrl` from [Chapter 2: CopilotKitProvider (React Component)](02_copilotkitprovider__react_component__.md)).

Let's look at a conceptual example of how you might set this up in a Node.js environment (like a Next.js API route or an Express server):

```typescript
// In your backend API endpoint (e.g., pages/api/copilotkit/[[...params]].ts)
import { CopilotRuntime } from "@copilotkit/runtime";
import { OpenAIAdapter } from "@copilotkit/runtime"; // Or other adapters
import { copilotRuntimeNodeHttpEndpoint } from "@copilotkit/runtime"; // Helper for Node.js

// 1. Instantiate CopilotRuntime with your backend actions, agents, etc.
const runtime = new CopilotRuntime({
  actions: [ /* your backend actions if any */ ],
  // agents: [ /* your backend agents if any */ ],
});

// 2. Choose and instantiate your Service Adapter
// Make sure you have the necessary API key set as an environment variable!
// For OpenAI (e.g., OPENAI_API_KEY)
const serviceAdapter = new OpenAIAdapter({ model: "gpt-4o" });

// For Anthropic (e.g., ANTHROPIC_API_KEY)
// import { AnthropicAdapter } from "@copilotkit/runtime";
// const serviceAdapter = new AnthropicAdapter({ model: "claude-3-sonnet-20240229" });

// For Google Gemini (e.g., GOOGLE_API_KEY)
// import { GoogleGenerativeAIAdapter } from "@copilotkit/runtime";
// const serviceAdapter = new GoogleGenerativeAIAdapter({ model: "gemini-1.5-pro" });

// 3. Create an HTTP handler that uses both runtime and adapter
const handler = copilotRuntimeNodeHttpEndpoint({
  runtime,
  serviceAdapter,
  // other options like endpoint, etc.
});

export default handler; // Expose the handler
```
In this setup:
*   We create our `runtime` which knows about our application's specific AI capabilities (actions/agents).
*   We then choose a `serviceAdapter` (here, `OpenAIAdapter`). If we wanted to switch to Anthropic's Claude, we'd just instantiate `AnthropicAdapter` instead. The `runtime` itself doesn't need to change!
*   The `copilotRuntimeNodeHttpEndpoint` (or a similar helper for your framework, like `copilotKitNextJsEndpoint` from `@copilotkit/nextjs`) wires everything together. When a request comes from the frontend, this handler uses the `runtime` to prepare the AI task and the `serviceAdapter` to communicate with the chosen LLM.

**Input/Output:**
*   **Frontend (via `runtimeUrl`) sends a request:** e.g., user's message "What's the weather in London?".
*   **Backend Handler (`handler`):**
    *   Uses `runtime` to process the request (gather context, available actions).
    *   Uses `serviceAdapter` (e.g., `OpenAIAdapter`) to send this processed information to OpenAI's GPT-4.
*   **OpenAIAdapter:**
    *   Translates the request into OpenAI's API format.
    *   Sends it to OpenAI.
    *   Receives OpenAI's response (likely a stream of text).
    *   Translates this stream back into a format CopilotKit understands.
*   **Backend Handler (`handler`):** Streams the translated response back to the frontend.
*   **Frontend:** Displays the AI's answer, "The weather in London is..."

**Popular Service Adapters in CopilotKit**

CopilotKit comes with built-in adapters for several popular LLM providers:

*   **`OpenAIAdapter`**: For all OpenAI models like GPT-4, GPT-4o, GPT-3.5-turbo.
    *   Find it in: `CopilotKit/packages/runtime/src/service-adapters/openai/openai-adapter.ts`
    *   Typically requires the `OPENAI_API_KEY` environment variable.
    *   Usage: `new OpenAIAdapter({ model: "gpt-4o" })`

*   **`AnthropicAdapter`**: For Anthropic's Claude models (Claude 3 Opus, Sonnet, Haiku).
    *   Find it in: `CopilotKit/packages/runtime/src/service-adapters/anthropic/anthropic-adapter.ts`
    *   Typically requires the `ANTHROPIC_API_KEY` environment variable.
    *   Usage: `new AnthropicAdapter({ model: "claude-3-sonnet-20240229" })`

*   **`GoogleGenerativeAIAdapter`**: For Google's Gemini family of models.
    *   Find it in: `CopilotKit/packages/runtime/src/service-adapters/google/google-genai-adapter.ts`
    *   Typically requires a Google API key (often `GOOGLE_API_KEY`). This adapter internally uses the `LangChainAdapter`.
    *   Usage: `new GoogleGenerativeAIAdapter({ model: "gemini-1.5-pro" })`

*   **`GroqAdapter`**: For accessing various open-source models (like Llama 3, Mixtral) with very fast inference speeds via GroqCloud.
    *   Find it in: `CopilotKit/packages/runtime/src/service-adapters/groq/groq-adapter.ts`
    *   Requires a `GROQ_API_KEY` environment variable.
    *   Usage: `new GroqAdapter({ model: "llama3-70b-8192" })`

*   **`LangChainAdapter`**: A very flexible adapter that allows you to use any model or chain compatible with the LangChain.js library. This is powerful if you have custom LLM setups or want to use models not directly supported by other adapters.
    *   Find it in: `CopilotKit/packages/runtime/src/service-adapters/langchain/langchain-adapter.ts`
    *   You provide a `chainFn` function that contains your LangChain logic.
    *   Usage:
        ```typescript
        import { LangChainAdapter } from "@copilotkit/runtime";
        // import { ChatOpenAI } from "@langchain/openai"; // Example LangChain model

        const serviceAdapter = new LangChainAdapter({
          chainFn: async ({ messages, tools, model: modelName }) => {
            // const model = new ChatOpenAI({ modelName });
            // return model.bindTools(tools).stream(messages);
            // Your custom LangChain logic here...
            return "Hello from LangChain!"; // Simplest example
          },
        });
        ```

**Under the Hood: The `CopilotServiceAdapter` Interface**

All these different adapters work because they follow a common set of rules, or an "interface," defined by CopilotKit. This interface is called `CopilotServiceAdapter`.

You can see its definition in `CopilotKit/packages/runtime/src/service-adapters/service-adapter.ts`. The most important part of this interface is the `process` method:

```typescript
// Simplified from service-adapter.ts
export interface CopilotRuntimeChatCompletionRequest {
  eventSource: RuntimeEventSource; // For streaming events back
  messages: Message[]; // The conversation history
  actions: ActionInput[]; // Available tools/actions
  model?: string; // Optional model name override
  // ... other parameters like threadId, agentSession ...
}

export interface CopilotServiceAdapter {
  process(
    request: CopilotRuntimeChatCompletionRequest,
  ): Promise<CopilotRuntimeChatCompletionResponse>; // Response includes threadId etc.
}
```
Here's what happens:
1.  The [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) (via your HTTP endpoint handler) gathers all necessary information (user messages, available actions, etc.) into a `CopilotRuntimeChatCompletionRequest` object.
2.  It then calls the `process()` method of the *specific Service Adapter* you configured (e.g., `OpenAIAdapter.process()`).
3.  Inside the adapter's `process()` method:
    *   It takes the `request` data.
    *   It **translates** the `messages` and `actions` into the format expected by its target LLM (e.g., OpenAI's API format).
    *   It makes the actual API call to the LLM.
    *   It receives the LLM's response (usually a stream).
    *   It **translates** this LLM-specific stream back into a series of standard CopilotKit events (like "text started," "text content," "action requested," etc.). It uses the `eventSource` from the `request` to send these standardized events. These events are what we'll explore in the next chapter, [Runtime Events & Protocol](09_runtime_events___protocol_.md).

Let's look at a simplified conceptual flow for an `OpenAIAdapter`:

```typescript
// Highly simplified concept of OpenAIAdapter.process()
// Based on CopilotKit/packages/runtime/src/service-adapters/openai/openai-adapter.ts
import { randomUUID } from "@copilotkit/shared"; // For generating IDs

class SimplifiedOpenAIAdapter implements CopilotServiceAdapter {
  // openai: OpenAI; // Actual OpenAI client initialized in constructor

  async process(request: CopilotRuntimeChatCompletionRequest) {
    const { eventSource, messages, actions, model } = request;

    // 1. Translate CopilotKit data to OpenAI format
    const openaiMessages = messages.map(convertToOpenAIMessage); // Helper function
    const openaiTools = actions.map(convertActionInputToOpenAITool); // Helper function

    // 2. Call the OpenAI API (conceptual)
    const stream = await this.openai.chat.completions.create({
      model: model || "gpt-4o", // Use provided model or default
      messages: openaiMessages,
      tools: openaiTools.length > 0 ? openaiTools : undefined,
      stream: true,
    });

    // 3. Translate OpenAI stream to CopilotKit events using eventSource
    eventSource.stream(async (eventStream$) => { // eventStream$ is how we send events
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        const toolCall = chunk.choices[0]?.delta?.tool_calls?.[0];

        if (content) {
          // eventStream$.sendTextMessageStartIfNotStartedYet();
          eventStream$.sendTextMessageContent({ messageId: chunk.id, content });
        }
        if (toolCall?.function?.name) {
          // eventStream$.sendActionExecutionStart(...);
        }
        // ... and other event types for arguments, end of messages/actions ...
      }
      eventStream$.complete(); // Signal end of all events
    });

    return { threadId: request.threadId || randomUUID() };
  }
  // Helper methods like convertToOpenAIMessage, convertActionInputToOpenAITool would exist
}
```
This `process` method is the heart of each adapter, doing the crucial translation work.

**Visualizing the Flow**

Here's a sequence diagram showing how a Service Adapter fits into the request lifecycle:

```mermaid
sequenceDiagram
    participant FrontendApp as Frontend Application
    participant BackendHandler as Backend HTTP Handler
    participant CRuntime as CopilotRuntime Engine
    participant SAdapter as Your Chosen Service Adapter (e.g., OpenAIAdapter)
    participant ExtLLM as External LLM API (e.g., OpenAI)

    FrontendApp->>BackendHandler: Sends user message + context (via runtimeUrl)
    BackendHandler->>CRuntime: Passes request to CopilotRuntime
    CRuntime->>SAdapter: Calls adapter.process(copilotkitRequest)
    SAdapter->>SAdapter: Translates request to LLM-specific format
    SAdapter->>ExtLLM: Sends API request to LLM
    ExtLLM-->>SAdapter: Streams LLM-specific response
    SAdapter->>SAdapter: Translates LLM response to CopilotKit events
    SAdapter-->>CRuntime: Streams CopilotKit events (via eventSource)
    CRuntime-->>BackendHandler: Forwards event stream
    BackendHandler-->>FrontendApp: Streams events back to frontend
    FrontendApp->>FrontendApp: Updates UI based on events
```

**Conclusion**

Service Adapters are the unsung heroes that give CopilotKit its flexibility in choosing LLMs. They act as translators, allowing your main application logic in the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) to remain consistent while effortlessly switching between different AI "brains" like OpenAI's GPT, Anthropic's Claude, Google's Gemini, or models on Groq.

Key Takeaways:
*   Service Adapters connect CopilotKit to various Large Language Models.
*   Each adapter translates requests and responses between CopilotKit's standard format and the LLM's specific API.
*   You configure your backend to use a specific adapter (e.g., `OpenAIAdapter`, `AnthropicAdapter`).
*   This makes it easy to switch LLM providers without major code changes.
*   All adapters implement the `CopilotServiceAdapter` interface, primarily its `process` method.

Now that we understand how the backend talks to different LLMs, you might be curious about the exact "language" or format of messages and events that flow between the frontend, the CopilotRuntime, and the Service Adapters. That's what we'll explore in the next chapter: [Runtime Events & Protocol](09_runtime_events___protocol_.md).

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="09_runtime_events___protocol_.md">
# Chapter 9: Runtime Events & Protocol

Welcome to the final chapter of our core tutorial! In [Chapter 8: Service Adapters (Backend)](08_service_adapters__backend__.md), we saw how CopilotKit can flexibly connect to different Large Language Models (LLMs) like OpenAI's GPT or Anthropic's Claude. This is crucial for choosing the "brain" of your AI.

But how do all the different pieces of CopilotKit – your frontend UI, your backend [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md), and the various [Service Adapters (Backend)](08_service_adapters__backend__.md) – actually *talk* to each other? Especially when information isn't just a single request and response, but a continuous flow, like an AI typing out a message word by word?

That's where **Runtime Events & Protocol** come in. This is the underlying "language" and set of rules they all agree to use.

**What's the Big Idea? A Common Language for AI Chatter**

Imagine you have people from different countries trying to collaborate on a project. They need a common language (like English) and some rules for conversation (like taking turns to speak) to understand each other.

Similarly, different parts of CopilotKit need a common way to exchange information.
*   Your **frontend** (e.g., a chat window) needs to tell the backend what the user said.
*   The **backend runtime engine** needs to tell the frontend that the AI is starting to "type" a response.
*   The **service adapter** (talking to the LLM) needs to send back chunks of the AI's generated text as they arrive.
*   The backend might need to tell the frontend that the AI wants to execute a specific [Actions (Frontend & Backend)](01_actions__frontend___backend__.md).

The **Runtime Events & Protocol** define this common language and the rules for how messages, action requests, state updates, and streaming content are structured and exchanged.

**What Problem Does It Solve? Enabling Complex, Real-time Communication**

Standard web communication often involves a simple request (e.g., "give me this webpage") and a single response. But AI interactions are more dynamic:
1.  **Streaming Content:** AI responses often arrive in pieces (tokens or words) to give the user a sense of real-time interaction, rather than waiting for the entire message.
2.  **Tool/Action Calls:** An LLM might decide mid-conversation that it needs to use a tool (an Action). This decision needs to be communicated.
3.  **Multi-step Processes:** [Agents (Backend Focus)](07_agents__backend_focus__.md) can involve many steps, and the UI might need updates on the agent's progress or internal state.

A simple request-response model isn't enough. We need a way to send multiple "updates" or "events" over a single connection. The Runtime Events & Protocol provide this structured way.

**Key Concepts: Events and Streaming**

*   **Events:** These are discrete pieces of information that represent something specific happening. Each event has a `type` (like "AI started typing a message") and some data related to that event (like the ID of the message being typed).
*   **Protocol:** This refers to the defined structure of these events (what fields they have, what the types mean) and how they are typically sent (e.g., as JSON objects).
*   **Streaming:** Instead of waiting for all information to be ready, events are often "streamed" – sent one by one as they occur. This is how you see an AI's message appear word by word.

**Core Runtime Event Types**

Let's look at some of the most common types of events that flow through the CopilotKit system. You'll find these defined in both the Python SDK (in `sdk-python/copilotkit/protocol.py`) and the TypeScript runtime (in `CopilotKit/packages/runtime/src/service-adapters/events.ts`).

Developers using CopilotKit typically don't need to *create* these events manually. They are generated by components like [Service Adapters (Backend)](08_service_adapters__backend__.md) and consumed by frontend hooks or UI components. Understanding them helps you see "under the hood."

Here are a few important ones:

1.  **`TextMessageStart`**:
    *   **Meaning:** Signals that the AI is about to start sending a new text message.
    *   **Key Data:** `messageId` (a unique ID for this message), `parentMessageId` (if it's a reply).
    *   **Example (conceptual JSON):**
        ```json
        { "type": "TextMessageStart", "messageId": "msg_abc123" }
        ```

2.  **`TextMessageContent`**:
    *   **Meaning:** Contains a piece (a "chunk") of the AI's text message.
    *   **Key Data:** `messageId` (to link it to the correct message), `content` (the actual text chunk).
    *   **Example:**
        ```json
        { "type": "TextMessageContent", "messageId": "msg_abc123", "content": "Hello " }
        ```
        (Another event might follow with `content: "world!"`)

3.  **`TextMessageEnd`**:
    *   **Meaning:** Signals that the AI has finished sending all chunks for this particular text message.
    *   **Key Data:** `messageId`.
    *   **Example:**
        ```json
        { "type": "TextMessageEnd", "messageId": "msg_abc123" }
        ```

4.  **`ActionExecutionStart`**:
    *   **Meaning:** The AI has decided to call an [Actions (Frontend & Backend)](01_actions__frontend___backend__.md) (a "tool").
    *   **Key Data:** `actionExecutionId` (unique ID for this call), `actionName` (name of the action to run), `parentMessageId`.
    *   **Example:**
        ```json
        {
          "type": "ActionExecutionStart",
          "actionExecutionId": "exec_xyz789",
          "actionName": "searchWeb"
        }
        ```

5.  **`ActionExecutionArgs`**:
    *   **Meaning:** Contains a chunk of the arguments (parameters) for the action being called. Arguments might also be streamed if they are large.
    *   **Key Data:** `actionExecutionId`, `args` (a string chunk of the JSON arguments).
    *   **Example:**
        ```json
        {
          "type": "ActionExecutionArgs",
          "actionExecutionId": "exec_xyz789",
          "args": "{\"query\":\"latest AI news\""
        }
        ```
        (Another event might follow with `args: "}"` to complete the JSON.)

6.  **`ActionExecutionEnd`**:
    *   **Meaning:** The AI has finished specifying all arguments for the action call.
    *   **Key Data:** `actionExecutionId`.
    *   **Example:**
        ```json
        { "type": "ActionExecutionEnd", "actionExecutionId": "exec_xyz789" }
        ```

7.  **`ActionExecutionResult`**:
    *   **Meaning:** This event carries the result *after* an action has been executed (either on the frontend or backend). This result is often sent back to the LLM so it can continue the conversation.
    *   **Key Data:** `actionName`, `actionExecutionId`, `result` (the output from the action).
    *   **Example:**
        ```json
        {
          "type": "ActionExecutionResult",
          "actionName": "searchWeb",
          "actionExecutionId": "exec_xyz789",
          "result": "[{\"title\": \"AI Breakthrough...\", ...}]"
        }
        ```

8.  **`AgentStateMessage`**:
    *   **Meaning:** Provides an update on the state of a backend [Agents (Backend Focus)](07_agents__backend_focus__.md).
    *   **Key Data:** `threadId`, `agentName`, `nodeName` (if using LangGraph), `runId`, `state` (current state data as JSON string), `running` (boolean).
    *   **Example:**
        ```json
        {
          "type": "AgentStateMessage",
          "threadId": "thread_123",
          "agentName": "researchAssistant",
          "nodeName": "webSearchNode",
          "runId": "run_456",
          "active": true,
          "role": "assistant", /* or agent specific role */
          "state": "{\"status\":\"Searching Google for 'CopilotKit'\"}",
          "running": true
        }
        ```

These are just some of the primary events. The full list (like `RunStarted`, `RunFinished`, `MetaEvent`) can be found in the SDK files mentioned.

**How It Works: The Flow of Events**

Let's trace a simplified scenario: a user asks a question in a chat UI, and the AI responds with streaming text.

```mermaid
sequenceDiagram
    participant User
    participant FrontendUI as Frontend UI (React)
    participant BackendRuntime as CopilotRuntime (Backend)
    participant ServiceAdapter as LLM Service Adapter
    participant LLM

    User->>FrontendUI: Types "Hi AI!"
    FrontendUI->>BackendRuntime: Sends user message (e.g., via HTTP POST to runtimeUrl)
    BackendRuntime->>ServiceAdapter: process(userMessage, availableActions)
    ServiceAdapter->>LLM: Sends request in LLM-specific format
    LLM-->>ServiceAdapter: Streams back AI response (LLM-specific format)

    Note over ServiceAdapter: Adapter translates LLM stream into CopilotKit Events

    ServiceAdapter->>BackendRuntime: Emits RuntimeEvent: `TextMessageStart`
    BackendRuntime-->>FrontendUI: Streams `TextMessageStart` event

    ServiceAdapter->>BackendRuntime: Emits RuntimeEvent: `TextMessageContent` (chunk 1)
    BackendRuntime-->>FrontendUI: Streams `TextMessageContent` event
    FrontendUI->>FrontendUI: Updates UI with chunk 1

    ServiceAdapter->>BackendRuntime: Emits RuntimeEvent: `TextMessageContent` (chunk 2)
    BackendRuntime-->>FrontendUI: Streams `TextMessageContent` event
    FrontendUI->>FrontendUI: Updates UI with chunk 2

    ServiceAdapter->>BackendRuntime: Emits RuntimeEvent: `TextMessageEnd`
    BackendRuntime-->>FrontendUI: Streams `TextMessageEnd` event
    FrontendUI->>FrontendUI: Finalizes message display
```

1.  **User Input:** The user sends a message.
2.  **To Backend:** The frontend sends this to the [CopilotRuntime (Backend Engine)](06_copilotruntime__backend_engine__.md) (this initial request is usually a standard HTTP request).
3.  **LLM Interaction:** The backend runtime uses a [Service Adapters (Backend)](08_service_adapters__backend__.md) to talk to an LLM.
4.  **Adapter Translation (LLM to Events):** As the LLM streams its response back to the Service Adapter, the adapter's job is to translate this LLM-specific stream into a series of standard CopilotKit Runtime Events. For example, it would generate `TextMessageStart`, then multiple `TextMessageContent` events for each piece of text, and finally `TextMessageEnd`.
5.  **Streaming to Frontend:** The backend runtime takes these events and streams them back to the frontend.
    *   For the JavaScript/TypeScript stack, this streaming is often handled via the GraphQL layer. The main JS runtime uses GraphQL, and a resolver function (like in `CopilotKit/packages/runtime/src/graphql/resolvers/copilot.resolver.ts`) uses a mechanism called `Repeater` to send these events as they arrive. This could be over a WebSocket, Server-Sent Events (SSE), or chunked HTTP responses, depending on the GraphQL client/server setup.
    *   The Python backend can also stream these events, typically as newline-delimited JSON strings over an HTTP connection.
6.  **Frontend Updates UI:** Frontend components (like `CopilotSidebar` from [UI Components (`@copilotkit/react-ui`)](04_ui_components____copilotkit_react_ui___.md) or custom ones using `useCopilotChat` from [Frontend Hooks (`@copilotkit/react-core`)](03_frontend_hooks____copilotkit_react_core___.md)) listen for these incoming events. When a `TextMessageContent` event arrives, the UI appends the new text to the current message bubble, making it look like the AI is typing.

**A Peek at the Definitions**

Let's see how these events are defined in code. You usually don't write this code, but seeing it helps understand the structure.

**Python SDK (`sdk-python/copilotkit/protocol.py`)**

The Python SDK defines event types using an `Enum` and structures using `TypedDict`.

```python
# From sdk-python/copilotkit/protocol.py

class RuntimeEventTypes(Enum):
    TEXT_MESSAGE_START = "TextMessageStart"
    TEXT_MESSAGE_CONTENT = "TextMessageContent"
    # ... other event types ...

class TextMessageContent(TypedDict):
    type: Literal[RuntimeEventTypes.TEXT_MESSAGE_CONTENT] # The event type
    messageId: str                                      # Links to a specific message
    content: str                                        # The actual text
```
This snippet shows the `RuntimeEventTypes` enum and the `TypedDict` for `TextMessageContent`. Other events like `ActionExecutionStart` are defined similarly. The SDK also provides utility functions (like `text_message_content(...)`) to create these event dictionaries easily. These dictionaries are then typically converted to JSON strings (often one event per line) for streaming.

**TypeScript Runtime (`CopilotKit/packages/runtime/src/service-adapters/events.ts`)**

The TypeScript runtime uses string enums and type aliases.

```typescript
// From CopilotKit/packages/runtime/src/service-adapters/events.ts

export enum RuntimeEventTypes {
  TextMessageStart = "TextMessageStart",
  TextMessageContent = "TextMessageContent",
  // ... other event types ...
}

export type RuntimeEvent =
  // ...
  | {
      type: RuntimeEventTypes.TextMessageContent; // The event type
      messageId: string;                          // Links to a specific message
      content: string;                            // The actual text
    }
  // ... other event type definitions ...
```
Here, `RuntimeEventTypes` is an enum, and `RuntimeEvent` is a union type that includes all possible event structures. The `RuntimeEventSource` class in this file has methods like `sendTextMessageContent()` that allow different parts of the backend (especially [Service Adapters (Backend)](08_service_adapters__backend__.md)) to easily create and emit these events onto a stream.

**The GraphQL Connection (for JavaScript/TypeScript Frontend)**

For applications using the JavaScript/TypeScript frontend libraries (`@copilotkit/react-core`, `@copilotkit/react-ui`), the primary way the frontend receives these streamed events from the backend JS runtime is through GraphQL.

*   **GraphQL Types:** The backend defines GraphQL types that mirror these runtime events (see `CopilotKit/packages/runtime/src/graphql/types/copilot-response.type.ts`). For example, `TextMessageOutput` in GraphQL corresponds to the `TextMessageStart/Content/End` runtime events.
*   **GraphQL Resolver:** A GraphQL resolver function (in `CopilotKit/packages/runtime/src/graphql/resolvers/copilot.resolver.ts`) is responsible for handling the AI request. When it gets events from the `RuntimeEventSource`, it pushes them into a `Repeater` object.
*   **Streaming via Repeater:** The `Repeater` allows GraphQL to stream these messages/events to the client as they become available. The frontend GraphQL client receives these events and updates the UI.

This GraphQL layer provides a well-defined API contract for the JavaScript frontend to consume the stream of runtime events.

**Conclusion**

The Runtime Events & Protocol are the lifeblood of communication within CopilotKit. They define a common "language" that allows your frontend, backend, and various AI services to exchange information in a structured way, enabling complex, real-time interactions like streaming text, tool usage by the AI, and agent state updates.

Key Takeaways:
*   CopilotKit uses a defined set of **Runtime Events** (like `TextMessageStart`, `ActionExecutionStart`) as its internal communication language.
*   This **Protocol** allows different parts of the system (frontend, backend, adapters) to understand each other.
*   **Streaming** of these events enables real-time features like live message updates.
*   Service Adapters translate LLM-specific outputs into these standard events.
*   Frontend libraries consume these events to update the UI. For JavaScript frontends, this is often facilitated by a GraphQL layer in the backend JS runtime.

While you, as a developer, might not often construct these events by hand, understanding that they exist and what they represent helps demystify how CopilotKit achieves its dynamic AI capabilities. This knowledge can be particularly useful if you ever need to debug interactions or extend CopilotKit in advanced ways.

This concludes our core journey through CopilotKit! You've learned about actions, providers, hooks, UI components, the textarea, the backend runtime, agents, service adapters, and now, the events that tie them all together. You're well-equipped to start building amazing AI-powered applications!

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

<file path="index.md">
# Tutorial: CopilotKit

CopilotKit is a **framework** for building *AI-powered assistants* (copilots) directly into your web applications.
It provides tools for both the *frontend* user interface (like **CopilotKitProvider**, **UI Components**, **CopilotTextarea**, and **Frontend Hooks**) and the *backend* logic (such as **CopilotRuntime** and **Service Adapters** for connecting to AI models).
Developers can define specific **Actions** (tasks the AI can perform) and more complex, stateful **Agents** (specialized AI helpers).
The entire system uses a defined **Runtime Events & Protocol** to allow all these parts to communicate seamlessly, enabling the AI to understand application context and execute tasks effectively.


**Source Repository:** [https://github.com/CopilotKit/CopilotKit.git](https://github.com/CopilotKit/CopilotKit.git)

```mermaid
flowchart TD
    A0["CopilotKitProvider (React Component)
"]
    A1["CopilotRuntime (Backend Engine)
"]
    A2["Actions (Frontend & Backend)
"]
    A3["Agents (Backend Focus)
"]
    A4["Service Adapters (Backend)
"]
    A5["Frontend Hooks (`@copilotkit/react-core`)
"]
    A6["Runtime Events & Protocol
"]
    A7["UI Components (`@copilotkit/react-ui`)
"]
    A8["CopilotTextarea (`@copilotkit/react-textarea`)
"]
    A0 -- "Provides context to" --> A5
    A0 -- "Connects frontend to" --> A1
    A1 -- "Uses for LLM access" --> A4
    A1 -- "Executes backend actions" --> A2
    A1 -- "Manages backend agents" --> A3
    A1 -- "Uses for communication" --> A6
    A5 -- "Defines client-side actions" --> A2
    A3 -- "Utilizes actions" --> A2
    A5 -- "Interacts with backend agents" --> A3
    A7 -- "Requires context from" --> A0
    A7 -- "Utilizes for functionality" --> A5
    A8 -- "Requires context from" --> A0
    A8 -- "Uses for suggestions" --> A5
    A3 -- "Streams state via" --> A6
    A5 -- "Consumes events via" --> A6
```

## Chapters

1. [Actions (Frontend & Backend)
](01_actions__frontend___backend__.md)
2. [CopilotKitProvider (React Component)
](02_copilotkitprovider__react_component__.md)
3. [Frontend Hooks (`@copilotkit/react-core`)
](03_frontend_hooks____copilotkit_react_core___.md)
4. [UI Components (`@copilotkit/react-ui`)
](04_ui_components____copilotkit_react_ui___.md)
5. [CopilotTextarea (`@copilotkit/react-textarea`)
](05_copilottextarea____copilotkit_react_textarea___.md)
6. [CopilotRuntime (Backend Engine)
](06_copilotruntime__backend_engine__.md)
7. [Agents (Backend Focus)
](07_agents__backend_focus__.md)
8. [Service Adapters (Backend)
](08_service_adapters__backend__.md)
9. [Runtime Events & Protocol
](09_runtime_events___protocol_.md)


---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)
</file>

</files>
